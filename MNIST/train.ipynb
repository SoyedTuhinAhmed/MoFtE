{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np\n",
    "import math\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.use_deterministic_algorithms(True)  # Ensures deterministic behavior\n",
    "    torch.manual_seed(seed)  # Seed for PyTorch (CPU)\n",
    "    torch.cuda.manual_seed(seed)  # Seed for PyTorch (single GPU)\n",
    "    torch.cuda.manual_seed_all(seed)  # Seed for PyTorch (all GPUs, if applicable)\n",
    "    np.random.seed(seed)  # Seed for NumPy\n",
    "    random.seed(seed)  # Seed for Python random\n",
    "    # For compatibility with older PyTorch versions:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the MNIST training set\n",
    "mnist_data = datasets.MNIST(\"../\", download=False, train=True,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Load the MNIST test set\n",
    "mnist_test = datasets.MNIST(\"../\", download=False, train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Split dataset into training and validation and create data loaders\n",
    "batch_size = 256\n",
    "ds_train, ds_val = torch.utils.data.random_split(mnist_data, [int(0.8 * len(mnist_data)), len(mnist_data) - int(0.8 * len(mnist_data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class LeNet5BatchNorm(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5BatchNorm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.bn1 = nn.BatchNorm2d((6), affine=True)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.bn2 = nn.BatchNorm2d((16), affine=True)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.bn3 = nn.BatchNorm1d(120, affine=True)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.bn4 = nn.BatchNorm1d(84, affine=True)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.bn3(self.fc1(x)))\n",
    "        x = F.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class LeNet5RMSNorm(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5RMSNorm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.bn1 = nn.RMSNorm((6, 24, 24), elementwise_affine=True)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.bn2 = nn.RMSNorm((16, 8, 8), elementwise_affine=True)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.bn3 = nn.RMSNorm((120), elementwise_affine=True)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.bn4 = nn.RMSNorm((84), elementwise_affine=True)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.bn3(self.fc1(x)))\n",
    "        x = F.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class LeNet5InstanceNorm(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LeNet5InstanceNorm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.bn1 = nn.InstanceNorm2d((6), affine=True)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.bn2 = nn.InstanceNorm2d((16), affine=True)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.bn3 = nn.LayerNorm(120, elementwise_affine=True)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.bn4 = nn.LayerNorm(84, elementwise_affine=True)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.bn3(self.fc1(x)))\n",
    "        x = F.relu(self.bn4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class LoRA2d(nn.Module):\n",
    "    \"\"\"\n",
    "    A Conv2d layer with an added LoRA (Low-Rank Adaptation) update.\n",
    "    \n",
    "    The forward pass computes:\n",
    "    \n",
    "        y = conv(x) + scaling * (lora_up(lora_down(x)))\n",
    "    \n",
    "    where:\n",
    "      - conv is a standard convolution (whose weights can optionally be frozen),\n",
    "      - lora_down is a convolution that reduces the input channels to `rank` (using the same\n",
    "        kernel size, stride, padding, etc. as `conv`),\n",
    "      - lora_up is a 1x1 convolution that maps from `rank` channels back to `out_channels`,\n",
    "      - scaling = lora_alpha / rank.\n",
    "      \n",
    "    Typically, lora_up is initialized to zero so that initially the module behaves exactly\n",
    "    like the original conv.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        groups=1,\n",
    "        rank=4,\n",
    "        lora_alpha=1.0,\n",
    "        lora_dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.lora_alpha = lora_alpha\n",
    "        # scaling factor: note that if rank==0, we could choose to skip the LoRA branch.\n",
    "        self.scaling = lora_alpha / rank if rank > 0 else 1.0\n",
    "        \n",
    "        # Optional dropout applied before the LoRA branch\n",
    "        self.lora_dropout = nn.Dropout2d(lora_dropout) if lora_dropout > 0.0 else nn.Identity()\n",
    "        \n",
    "        if rank > 0:\n",
    "            # LoRA \"down\" layer: uses the same kernel size and other hyperparameters so that the \n",
    "            # sliding-window (patch) structure is preserved.\n",
    "            self.lora_down = nn.Conv2d(\n",
    "                in_channels, rank, kernel_size,\n",
    "                stride=stride, padding=padding, dilation=dilation,\n",
    "                groups=groups, bias=False\n",
    "            )\n",
    "            # LoRA \"up\" layer: a 1x1 convolution to bring the channels back to out_channels.\n",
    "            self.lora_up = nn.Conv2d(rank, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "            \n",
    "            # Initialize the LoRA layers:\n",
    "            # It is common to initialize the up-projection to zero so that initially the LoRA branch\n",
    "            # contributes nothing.\n",
    "            nn.init.kaiming_uniform_(self.lora_down.weight, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_up.weight)\n",
    "        else:\n",
    "            # If rank == 0, the LoRA branch is omitted.\n",
    "            self.lora_down = None\n",
    "            self.lora_up = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Add LoRA update if enabled.\n",
    "        if self.rank > 0:\n",
    "            # Optionally apply dropout on the input.\n",
    "            lora_x = self.lora_dropout(x)\n",
    "            # Compute the low-rank update.\n",
    "            lora_update = self.lora_up(self.lora_down(lora_x))\n",
    "            # Scale and add to the original conv output.\n",
    "            y = self.scaling * lora_update\n",
    "        \n",
    "        return y\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    \"\"\"\n",
    "    A Linear layer with an added LoRA (Low-Rank Adaptation) update.\n",
    "\n",
    "    The forward pass computes:\n",
    "\n",
    "        y = linear(x) + scaling * (lora_up(lora_down(x)))\n",
    "\n",
    "    where:\n",
    "      - linear is the standard linear layer,\n",
    "      - lora_down is a linear layer mapping from `in_features` to `rank` (with no bias),\n",
    "      - lora_up is a linear layer mapping from `rank` to `out_features` (with no bias),\n",
    "      - scaling = lora_alpha / rank.\n",
    "\n",
    "    Typically, lora_up is initialized to zeros so that initially the module behaves like\n",
    "    the original linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        rank: int = 4,\n",
    "        lora_alpha: float = 1.0,\n",
    "        lora_dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rank = rank\n",
    "        self.lora_alpha = lora_alpha\n",
    "        # The scaling factor ensures the LoRA branch is properly scaled.\n",
    "        self.scaling = lora_alpha / rank if rank > 0 else 1.0\n",
    "        # Optional dropout for the LoRA branch.\n",
    "        self.lora_dropout = nn.Dropout(lora_dropout) if lora_dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        if rank > 0:\n",
    "            # LoRA \"down\" projection: from in_features -> rank (no bias)\n",
    "            self.lora_down = nn.Linear(in_features, rank, bias=False)\n",
    "            # LoRA \"up\" projection: from rank -> out_features (no bias)\n",
    "            self.lora_up = nn.Linear(rank, out_features, bias=False)\n",
    "            # Initialize lora_down using kaiming initialization\n",
    "            nn.init.kaiming_uniform_(self.lora_down.weight, a=math.sqrt(5))\n",
    "            # Initialize lora_up to zeros so that the update starts off as zero.\n",
    "            nn.init.zeros_(self.lora_up.weight)\n",
    "        else:\n",
    "            self.lora_down = None\n",
    "            self.lora_up = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Standard linear transformation.\n",
    "        if self.rank > 0:\n",
    "            # Apply dropout (if any) to the input.\n",
    "            lora_x = self.lora_dropout(x)\n",
    "            # Compute the low-rank update.\n",
    "            lora_update = self.lora_up(self.lora_down(lora_x))\n",
    "            # Add the scaled update to the original output.\n",
    "            y = self.scaling * lora_update\n",
    "        return y\n",
    "    \n",
    "class LeNet5Adapter(nn.Module):\n",
    "    def __init__(self, num_classes=10, rank=6):\n",
    "        super(LeNet5Adapter, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.lora1 = LoRA2d(self.conv1.in_channels, self.conv1.out_channels, self.conv1.kernel_size,\n",
    "                            self.conv1.stride, self.conv1.padding, self.conv1.dilation, self.conv1.groups,\n",
    "                            rank=rank, lora_alpha=1, lora_dropout=0)\n",
    "        self.bn1 = nn.InstanceNorm2d(6, affine=True)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "        self.lora2 = LoRA2d(self.conv2.in_channels, self.conv2.out_channels, self.conv2.kernel_size,\n",
    "                            self.conv2.stride, self.conv2.padding, self.conv2.dilation, self.conv2.groups,\n",
    "                            rank=rank, lora_alpha=1, lora_dropout=0)\n",
    "        self.bn2 = nn.InstanceNorm2d(16, affine=True)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120)\n",
    "        self.lora3 = LoRALinear(self.fc1.in_features, self.fc1.out_features,\n",
    "                            rank=rank, lora_alpha=1, lora_dropout=0)\n",
    "        \n",
    "        self.bn3 = nn.LayerNorm(120, elementwise_affine=True)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.lora4 = LoRALinear(self.fc2.in_features, self.fc2.out_features,\n",
    "                            rank=rank, lora_alpha=1, lora_dropout=0)\n",
    "        self.bn4 = nn.LayerNorm(84, elementwise_affine=True)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "        self.lora5 = LoRALinear(self.fc3.in_features, self.fc3.out_features,\n",
    "                            rank=rank, lora_alpha=1, lora_dropout=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) + self.lora1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.conv2(x) + self.lora2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = self.fc1(x) + self.lora3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "\n",
    "        x = self.fc2(x) + self.lora4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.fc3(x) + self.lora5(x)\n",
    "        return x\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "from simulator.algorithms.dnn.torch.convert import synchronize\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, num_epochs, scheduler, train_dataset, val_dataset,\n",
    "                 batch_size=128, device=None, criterion=None, best_model_path='best_model.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: the PyTorch model.\n",
    "            optimizer: optimizer for training.\n",
    "            num_epochs: total number of epochs for training.\n",
    "            scheduler: learning rate scheduler.\n",
    "            train_dataset: training dataset (instance of torch.utils.data.Dataset).\n",
    "            val_dataset: validation dataset (instance of torch.utils.data.Dataset).\n",
    "            batch_size: batch size for both train and val.\n",
    "            device: torch.device; if None it will use CUDA if available.\n",
    "            criterion: loss function; if None, defaults to torch.nn.CrossEntropyLoss.\n",
    "            best_model_path: file path to save the best model.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.num_epochs = num_epochs\n",
    "        self.scheduler = scheduler\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device or torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.criterion = criterion or torch.nn.CrossEntropyLoss()\n",
    "        self.best_model_path = best_model_path\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "        self.val_loader = DataLoader(val_dataset, batch_size=10000, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def validate(self, recalibrate=False):\n",
    "        \"\"\"Evaluates the model on the validation dataset.\n",
    "\n",
    "        Returns:\n",
    "            avg_loss (float): Average loss over the validation set.\n",
    "            accuracy (float): Accuracy over the validation set.\n",
    "        \"\"\"\n",
    "        if recalibrate:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # For classification tasks assuming outputs are logits\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += torch.sum(preds == labels).item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader.dataset)\n",
    "        accuracy = correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def train(self, crossSim = False, adapter=False):\n",
    "        \"\"\"Trains the model and evaluates it on the validation set each epoch.\n",
    "        \n",
    "        The best performing model (based on validation accuracy) is saved.\n",
    "        \"\"\"\n",
    "        best_acc = 0.0\n",
    "        for epoch in range(1, self.num_epochs + 1):\n",
    "            # Training Phase\n",
    "            if adapter:\n",
    "                self.model.eval()\n",
    "            else:\n",
    "                self.model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for inputs, labels in self.train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                if crossSim:\n",
    "                    synchronize(self.model)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            train_loss = running_loss / len(self.train_loader.dataset)\n",
    "            \n",
    "            # Validation Phase (using the validate method)\n",
    "            val_loss, val_acc = self.validate()\n",
    "\n",
    "            print(f\"Epoch [{epoch}/{self.num_epochs}] \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | \"\n",
    "                  f\"Val Loss: {val_loss:.4f} | \"\n",
    "                  f\"Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Step the scheduler if provided\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            # Save the best model based on validation accuracy\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(self.model.state_dict(), self.best_model_path)\n",
    "                print(f\"Saving best model with accuracy: {best_acc:.4f}\")\n",
    "\n",
    "        print(\"Training complete. Best validation accuracy: {:.4f}\".format(best_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5RMSNorm(num_classes=10)\n",
    "gpu_id = 0\n",
    "device = torch.device('cuda:'+str(gpu_id) if torch.cuda.is_available() else 'cpu')\n",
    "trainer = Trainer(model=model, optimizer=torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-5),\n",
    "                  num_epochs=100, scheduler=None, train_dataset=ds_train, val_dataset=ds_val, batch_size=128, \n",
    "                  device=device, criterion=nn.CrossEntropyLoss(), best_model_path='./trained_models/basline_mnist_RMSNorm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.2760 | Val Loss: 0.0812 | Val Acc: 0.9748\n",
      "Saving best model with accuracy: 0.9748\n",
      "Epoch [2/100] Train Loss: 0.0641 | Val Loss: 0.0576 | Val Acc: 0.9828\n",
      "Saving best model with accuracy: 0.9828\n",
      "Epoch [3/100] Train Loss: 0.0445 | Val Loss: 0.0458 | Val Acc: 0.9856\n",
      "Saving best model with accuracy: 0.9856\n",
      "Epoch [4/100] Train Loss: 0.0329 | Val Loss: 0.0506 | Val Acc: 0.9838\n",
      "Epoch [5/100] Train Loss: 0.0281 | Val Loss: 0.0397 | Val Acc: 0.9886\n",
      "Saving best model with accuracy: 0.9886\n",
      "Epoch [6/100] Train Loss: 0.0215 | Val Loss: 0.0377 | Val Acc: 0.9889\n",
      "Saving best model with accuracy: 0.9889\n",
      "Epoch [7/100] Train Loss: 0.0196 | Val Loss: 0.0422 | Val Acc: 0.9865\n",
      "Epoch [8/100] Train Loss: 0.0142 | Val Loss: 0.0394 | Val Acc: 0.9884\n",
      "Epoch [9/100] Train Loss: 0.0126 | Val Loss: 0.0367 | Val Acc: 0.9893\n",
      "Saving best model with accuracy: 0.9893\n",
      "Epoch [10/100] Train Loss: 0.0090 | Val Loss: 0.0405 | Val Acc: 0.9888\n",
      "Epoch [11/100] Train Loss: 0.0074 | Val Loss: 0.0440 | Val Acc: 0.9876\n",
      "Epoch [12/100] Train Loss: 0.0056 | Val Loss: 0.0412 | Val Acc: 0.9881\n",
      "Epoch [13/100] Train Loss: 0.0041 | Val Loss: 0.0426 | Val Acc: 0.9887\n",
      "Epoch [14/100] Train Loss: 0.0035 | Val Loss: 0.0453 | Val Acc: 0.9886\n",
      "Epoch [15/100] Train Loss: 0.0032 | Val Loss: 0.0412 | Val Acc: 0.9892\n",
      "Epoch [16/100] Train Loss: 0.0028 | Val Loss: 0.0421 | Val Acc: 0.9892\n",
      "Epoch [17/100] Train Loss: 0.0015 | Val Loss: 0.0425 | Val Acc: 0.9889\n",
      "Epoch [18/100] Train Loss: 0.0015 | Val Loss: 0.0403 | Val Acc: 0.9896\n",
      "Saving best model with accuracy: 0.9896\n",
      "Epoch [19/100] Train Loss: 0.0009 | Val Loss: 0.0403 | Val Acc: 0.9898\n",
      "Saving best model with accuracy: 0.9898\n",
      "Epoch [20/100] Train Loss: 0.0007 | Val Loss: 0.0408 | Val Acc: 0.9901\n",
      "Saving best model with accuracy: 0.9901\n",
      "Epoch [21/100] Train Loss: 0.0006 | Val Loss: 0.0416 | Val Acc: 0.9898\n",
      "Epoch [22/100] Train Loss: 0.0005 | Val Loss: 0.0416 | Val Acc: 0.9898\n",
      "Epoch [23/100] Train Loss: 0.0005 | Val Loss: 0.0412 | Val Acc: 0.9902\n",
      "Saving best model with accuracy: 0.9902\n",
      "Epoch [24/100] Train Loss: 0.0004 | Val Loss: 0.0426 | Val Acc: 0.9896\n",
      "Epoch [25/100] Train Loss: 0.0004 | Val Loss: 0.0422 | Val Acc: 0.9899\n",
      "Epoch [26/100] Train Loss: 0.0004 | Val Loss: 0.0425 | Val Acc: 0.9898\n",
      "Epoch [27/100] Train Loss: 0.0004 | Val Loss: 0.0425 | Val Acc: 0.9898\n",
      "Epoch [28/100] Train Loss: 0.0003 | Val Loss: 0.0424 | Val Acc: 0.9899\n",
      "Epoch [29/100] Train Loss: 0.0003 | Val Loss: 0.0430 | Val Acc: 0.9899\n",
      "Epoch [30/100] Train Loss: 0.0003 | Val Loss: 0.0437 | Val Acc: 0.9899\n",
      "Epoch [31/100] Train Loss: 0.0003 | Val Loss: 0.0432 | Val Acc: 0.9897\n",
      "Epoch [32/100] Train Loss: 0.0003 | Val Loss: 0.0433 | Val Acc: 0.9898\n",
      "Epoch [33/100] Train Loss: 0.0003 | Val Loss: 0.0435 | Val Acc: 0.9896\n",
      "Epoch [34/100] Train Loss: 0.0003 | Val Loss: 0.0435 | Val Acc: 0.9898\n",
      "Epoch [35/100] Train Loss: 0.0003 | Val Loss: 0.0436 | Val Acc: 0.9898\n",
      "Epoch [36/100] Train Loss: 0.0002 | Val Loss: 0.0439 | Val Acc: 0.9898\n",
      "Epoch [37/100] Train Loss: 0.0002 | Val Loss: 0.0437 | Val Acc: 0.9902\n",
      "Epoch [38/100] Train Loss: 0.0002 | Val Loss: 0.0440 | Val Acc: 0.9899\n",
      "Epoch [39/100] Train Loss: 0.0002 | Val Loss: 0.0441 | Val Acc: 0.9898\n",
      "Epoch [40/100] Train Loss: 0.0002 | Val Loss: 0.0442 | Val Acc: 0.9896\n",
      "Epoch [41/100] Train Loss: 0.0002 | Val Loss: 0.0442 | Val Acc: 0.9899\n",
      "Epoch [42/100] Train Loss: 0.0002 | Val Loss: 0.0442 | Val Acc: 0.9898\n",
      "Epoch [43/100] Train Loss: 0.0002 | Val Loss: 0.0446 | Val Acc: 0.9898\n",
      "Epoch [44/100] Train Loss: 0.0002 | Val Loss: 0.0443 | Val Acc: 0.9899\n",
      "Epoch [45/100] Train Loss: 0.0002 | Val Loss: 0.0445 | Val Acc: 0.9898\n",
      "Epoch [46/100] Train Loss: 0.0002 | Val Loss: 0.0444 | Val Acc: 0.9899\n",
      "Epoch [47/100] Train Loss: 0.0002 | Val Loss: 0.0446 | Val Acc: 0.9898\n",
      "Epoch [48/100] Train Loss: 0.0002 | Val Loss: 0.0444 | Val Acc: 0.9895\n",
      "Epoch [49/100] Train Loss: 0.0002 | Val Loss: 0.0447 | Val Acc: 0.9898\n",
      "Epoch [50/100] Train Loss: 0.0002 | Val Loss: 0.0447 | Val Acc: 0.9898\n",
      "Epoch [51/100] Train Loss: 0.0002 | Val Loss: 0.0447 | Val Acc: 0.9897\n",
      "Epoch [52/100] Train Loss: 0.0002 | Val Loss: 0.0450 | Val Acc: 0.9897\n",
      "Epoch [53/100] Train Loss: 0.0002 | Val Loss: 0.0448 | Val Acc: 0.9898\n",
      "Epoch [54/100] Train Loss: 0.0002 | Val Loss: 0.0450 | Val Acc: 0.9898\n",
      "Epoch [55/100] Train Loss: 0.0002 | Val Loss: 0.0449 | Val Acc: 0.9896\n",
      "Epoch [56/100] Train Loss: 0.0002 | Val Loss: 0.0452 | Val Acc: 0.9899\n",
      "Epoch [57/100] Train Loss: 0.0002 | Val Loss: 0.0449 | Val Acc: 0.9898\n",
      "Epoch [58/100] Train Loss: 0.0002 | Val Loss: 0.0451 | Val Acc: 0.9898\n",
      "Epoch [59/100] Train Loss: 0.0002 | Val Loss: 0.0453 | Val Acc: 0.9898\n",
      "Epoch [60/100] Train Loss: 0.0002 | Val Loss: 0.0451 | Val Acc: 0.9898\n",
      "Epoch [61/100] Train Loss: 0.0002 | Val Loss: 0.0453 | Val Acc: 0.9898\n",
      "Epoch [62/100] Train Loss: 0.0002 | Val Loss: 0.0453 | Val Acc: 0.9898\n",
      "Epoch [63/100] Train Loss: 0.0002 | Val Loss: 0.0452 | Val Acc: 0.9896\n",
      "Epoch [64/100] Train Loss: 0.0002 | Val Loss: 0.0454 | Val Acc: 0.9897\n",
      "Epoch [65/100] Train Loss: 0.0002 | Val Loss: 0.0455 | Val Acc: 0.9896\n",
      "Epoch [66/100] Train Loss: 0.0002 | Val Loss: 0.0452 | Val Acc: 0.9897\n",
      "Epoch [67/100] Train Loss: 0.0002 | Val Loss: 0.0454 | Val Acc: 0.9898\n",
      "Epoch [68/100] Train Loss: 0.0002 | Val Loss: 0.0453 | Val Acc: 0.9897\n",
      "Epoch [69/100] Train Loss: 0.0002 | Val Loss: 0.0453 | Val Acc: 0.9896\n",
      "Epoch [70/100] Train Loss: 0.0002 | Val Loss: 0.0454 | Val Acc: 0.9897\n",
      "Epoch [71/100] Train Loss: 0.0002 | Val Loss: 0.0456 | Val Acc: 0.9896\n",
      "Epoch [72/100] Train Loss: 0.0002 | Val Loss: 0.0457 | Val Acc: 0.9897\n",
      "Epoch [73/100] Train Loss: 0.0002 | Val Loss: 0.0456 | Val Acc: 0.9895\n",
      "Epoch [74/100] Train Loss: 0.0002 | Val Loss: 0.0455 | Val Acc: 0.9897\n",
      "Epoch [75/100] Train Loss: 0.0002 | Val Loss: 0.0456 | Val Acc: 0.9895\n",
      "Epoch [76/100] Train Loss: 0.0001 | Val Loss: 0.0457 | Val Acc: 0.9895\n",
      "Epoch [77/100] Train Loss: 0.0001 | Val Loss: 0.0457 | Val Acc: 0.9897\n",
      "Epoch [78/100] Train Loss: 0.0001 | Val Loss: 0.0459 | Val Acc: 0.9896\n",
      "Epoch [79/100] Train Loss: 0.0001 | Val Loss: 0.0458 | Val Acc: 0.9897\n",
      "Epoch [80/100] Train Loss: 0.0001 | Val Loss: 0.0457 | Val Acc: 0.9896\n",
      "Epoch [81/100] Train Loss: 0.0001 | Val Loss: 0.0457 | Val Acc: 0.9897\n",
      "Epoch [82/100] Train Loss: 0.0001 | Val Loss: 0.0457 | Val Acc: 0.9898\n",
      "Epoch [83/100] Train Loss: 0.0001 | Val Loss: 0.0456 | Val Acc: 0.9894\n",
      "Epoch [84/100] Train Loss: 0.0001 | Val Loss: 0.0458 | Val Acc: 0.9897\n",
      "Epoch [85/100] Train Loss: 0.0001 | Val Loss: 0.0458 | Val Acc: 0.9894\n",
      "Epoch [86/100] Train Loss: 0.0001 | Val Loss: 0.0459 | Val Acc: 0.9894\n",
      "Epoch [87/100] Train Loss: 0.0001 | Val Loss: 0.0459 | Val Acc: 0.9896\n",
      "Epoch [88/100] Train Loss: 0.0001 | Val Loss: 0.0459 | Val Acc: 0.9895\n",
      "Epoch [89/100] Train Loss: 0.0001 | Val Loss: 0.0458 | Val Acc: 0.9896\n",
      "Epoch [90/100] Train Loss: 0.0001 | Val Loss: 0.0459 | Val Acc: 0.9896\n",
      "Epoch [91/100] Train Loss: 0.0001 | Val Loss: 0.0459 | Val Acc: 0.9896\n",
      "Epoch [92/100] Train Loss: 0.0001 | Val Loss: 0.0458 | Val Acc: 0.9897\n",
      "Epoch [93/100] Train Loss: 0.0001 | Val Loss: 0.0459 | Val Acc: 0.9896\n",
      "Epoch [94/100] Train Loss: 0.0001 | Val Loss: 0.0459 | Val Acc: 0.9896\n",
      "Epoch [95/100] Train Loss: 0.0001 | Val Loss: 0.0459 | Val Acc: 0.9895\n",
      "Epoch [96/100] Train Loss: 0.0001 | Val Loss: 0.0461 | Val Acc: 0.9896\n",
      "Epoch [97/100] Train Loss: 0.0001 | Val Loss: 0.0461 | Val Acc: 0.9898\n",
      "Epoch [98/100] Train Loss: 0.0001 | Val Loss: 0.0463 | Val Acc: 0.9897\n",
      "Epoch [99/100] Train Loss: 0.0001 | Val Loss: 0.0460 | Val Acc: 0.9895\n",
      "Epoch [100/100] Train Loss: 0.0001 | Val Loss: 0.0462 | Val Acc: 0.9894\n",
      "Training complete. Best validation accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "torch.use_deterministic_algorithms(False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference simulation 1 of 100\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies:  0.8845\n",
      "accuracies:  0.8250833333333333\n",
      "accuracies:  0.88425 3 of 100\n",
      "accuracies:  0.9174166666666667\n",
      "accuracies:  0.8111666666666667\n",
      "accuracies:  0.8975833333333333\n",
      "accuracies:  0.877on 7 of 100\n",
      "accuracies:  0.9235833333333333\n",
      "accuracies:  0.8823333333333333\n",
      "accuracies:  0.8545n 10 of 100\n",
      "accuracies:  0.9313333333333333\n",
      "accuracies:  0.9066666666666666\n",
      "accuracies:  0.88225 13 of 100\n",
      "accuracies:  0.905on 14 of 100\n",
      "accuracies:  0.86325 15 of 100\n",
      "accuracies:  0.8841666666666667\n",
      "accuracies:  0.9429166666666666\n",
      "accuracies:  0.9494166666666667\n",
      "accuracies:  0.8188333333333333\n",
      "accuracies:  0.88425 20 of 100\n",
      "accuracies:  0.827on 21 of 100\n",
      "accuracies:  0.7763333333333333\n",
      "accuracies:  0.7355833333333334\n",
      "accuracies:  0.8901666666666667\n",
      "accuracies:  0.9328333333333333\n",
      "accuracies:  0.886on 26 of 100\n",
      "accuracies:  0.8605833333333334\n",
      "accuracies:  0.8944166666666666\n",
      "accuracies:  0.5953333333333334\n",
      "accuracies:  0.89175 30 of 100\n",
      "accuracies:  0.8279166666666666\n",
      "accuracies:  0.90675 32 of 100\n",
      "accuracies:  0.8724166666666666\n",
      "accuracies:  0.8980833333333333\n",
      "accuracies:  0.7345833333333334\n",
      "accuracies:  0.891on 36 of 100\n",
      "accuracies:  0.9103333333333333\n",
      "accuracies:  0.8431666666666666\n",
      "accuracies:  0.8355n 39 of 100\n",
      "accuracies:  0.8950833333333333\n",
      "accuracies:  0.95425 41 of 100\n",
      "accuracies:  0.8985833333333333\n",
      "accuracies:  0.8991666666666667\n",
      "accuracies:  0.92175 44 of 100\n",
      "accuracies:  0.95ion 45 of 100\n",
      "accuracies:  0.8839166666666667\n",
      "accuracies:  0.9045833333333333\n",
      "accuracies:  0.7294166666666667\n",
      "accuracies:  0.89375 49 of 100\n",
      "accuracies:  0.8903333333333333\n",
      "accuracies:  0.9273333333333333\n",
      "accuracies:  0.9155833333333333\n",
      "accuracies:  0.8584166666666667\n",
      "accuracies:  0.8293333333333334\n",
      "accuracies:  0.8456666666666667\n",
      "accuracies:  0.8505833333333334\n",
      "accuracies:  0.9280833333333334\n",
      "accuracies:  0.9213333333333333\n",
      "accuracies:  0.8921666666666667\n",
      "accuracies:  0.8541666666666666\n",
      "accuracies:  0.7835n 61 of 100\n",
      "accuracies:  0.7378333333333333\n",
      "accuracies:  0.91075 63 of 100\n",
      "accuracies:  0.9323333333333333\n",
      "accuracies:  0.92275 65 of 100\n",
      "accuracies:  0.9543333333333334\n",
      "accuracies:  0.9364166666666667\n",
      "accuracies:  0.7563333333333333\n",
      "accuracies:  0.9305n 69 of 100\n",
      "accuracies:  0.8888333333333334\n",
      "accuracies:  0.826on 71 of 100\n",
      "accuracies:  0.8246666666666667\n",
      "accuracies:  0.88725 73 of 100\n",
      "accuracies:  0.8980833333333333\n",
      "accuracies:  0.9165833333333333\n",
      "accuracies:  0.5834166666666667\n",
      "accuracies:  0.8155n 77 of 100\n",
      "accuracies:  0.8033333333333333\n",
      "accuracies:  0.9105833333333333\n",
      "accuracies:  0.8893333333333333\n",
      "accuracies:  0.86475 81 of 100\n",
      "accuracies:  0.86775 82 of 100\n",
      "accuracies:  0.8093333333333333\n",
      "accuracies:  0.8471666666666666\n",
      "accuracies:  0.7845n 85 of 100\n",
      "accuracies:  0.92425 86 of 100\n",
      "accuracies:  0.9456666666666667\n",
      "accuracies:  0.8938333333333334\n",
      "accuracies:  0.77175 89 of 100\n",
      "accuracies:  0.86425 90 of 100\n",
      "accuracies:  0.90625 91 of 100\n",
      "accuracies:  0.8939166666666667\n",
      "accuracies:  0.89525 93 of 100\n",
      "accuracies:  0.82225 94 of 100\n",
      "accuracies:  0.8420833333333333\n",
      "accuracies:  0.7723333333333333\n",
      "accuracies:  0.9578333333333333\n",
      "accuracies:  0.7839166666666667\n",
      "accuracies:  0.9104166666666667\n",
      "accuracies:  0.9305833333333333\n",
      "\n",
      "===========\n",
      "No analog errors during training, CrossSim analog errors during test\n",
      "Test accuracy: 86.78% +/- 6.716%\n"
     ]
    }
   ],
   "source": [
    "from applications.mvm_params import set_params\n",
    "from simulator.algorithms.dnn.torch.convert import from_torch, reinitialize\n",
    "\n",
    "# Create a parameters object that models a memory device with very large errors\n",
    "params_analog = set_params(weight_bits = 8, wtmodel = \"BALANCED\", \n",
    "                         error_model = \"generic\",\n",
    "                         proportional_error = True,\n",
    "                         alpha_error = 0.4, useGPU = True, gpu_id = gpu_id)\n",
    "\n",
    "# Convert the layers in the trained CNN\n",
    "model = LeNet5RMSNorm(num_classes=10)\n",
    "model.load_state_dict(torch.load('./trained_models/basline_mnist_RMSNorm.pth', weights_only=False), strict=True)\n",
    "analog_mnist_cnn_pt = from_torch(model.to(device), params_analog)\n",
    "trainer.model = analog_mnist_cnn_pt\n",
    "\n",
    "# Number of inference simulations with re-sampled random analog errors\n",
    "N_runs = 100\n",
    "\n",
    "# Perform analog inference on the test set\n",
    "accuracies = []\n",
    "# analog_mnist_cnn_pt.to(device)\n",
    "for i in range(N_runs):\n",
    "    print(\"Inference simulation {:d} of {:d}\".format(i+1,N_runs), end=\"\\r\")\n",
    "    acc = trainer.validate(recalibrate=False)[-1]\n",
    "    print('accuracies: ', acc)\n",
    "    accuracies.append(acc)\n",
    "    reinitialize(trainer.model)\n",
    "\n",
    "# Evaluate average test accuracy\n",
    "print('\\n===========')\n",
    "print('No analog errors during training, CrossSim analog errors during test')\n",
    "accuracy_digitalTrain_analogTest = np.mean(accuracies)\n",
    "std_digitalTrain_analogTest = np.std(accuracies)\n",
    "print('Test accuracy: {:.2f}% +/- {:.3f}%'.format(100*accuracy_digitalTrain_analogTest, 100*std_digitalTrain_analogTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5BatchNorm(num_classes=10)\n",
    "gpu_id = 0\n",
    "device = torch.device('cuda:'+str(gpu_id) if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(torch.load('./trained_models/basline_mnist.pth', weights_only=False), strict=False)\n",
    "params_analog = set_params(weight_bits = 8, wtmodel = \"BALANCED\", \n",
    "                         error_model = \"generic\",\n",
    "                         proportional_error = False,\n",
    "                         alpha_error = 0.06,\n",
    "                         useGPU = True,\n",
    "                         gpu_id = gpu_id)\n",
    "model = from_torch(model, params_analog)\n",
    "\n",
    "trainer = Trainer(model=model, optimizer=torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-5),\n",
    "                  num_epochs=100, scheduler=None, train_dataset=ds_train, val_dataset=ds_val, batch_size=128, \n",
    "                  device=device, criterion=nn.CrossEntropyLoss(), best_model_path='./trained_models/baseline_alpha0.06_mnist.pth')\n",
    "# trainer.train(crossSim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2024)\n",
    "\n",
    "# Create a parameters object that models a memory device with very large errors\n",
    "params_analog = set_params(weight_bits = 8, wtmodel = \"BALANCED\", \n",
    "                         error_model = \"generic\",\n",
    "                         proportional_error = False,\n",
    "                         alpha_error = 0.06,\n",
    "                         useGPU = True,\n",
    "                         gpu_id = gpu_id)\n",
    "\n",
    "# Convert the layers in the trained CNN\n",
    "model = LeNet5BatchNorm(num_classes=10)\n",
    "model.load_state_dict(torch.load('./trained_models/baseline_alpha0.06_mnist.pth', weights_only=False), strict=True)\n",
    "analog_mnist_cnn_pt = from_torch(model.to(device), params_analog)\n",
    "trainer.model = analog_mnist_cnn_pt\n",
    "\n",
    "# Number of inference simulations with re-sampled random analog errors\n",
    "N_runs = 10\n",
    "\n",
    "# Perform analog inference on the test set\n",
    "accuracies = []\n",
    "# analog_mnist_cnn_pt.to(device)\n",
    "for i in range(N_runs):\n",
    "    print(\"Inference simulation {:d} of {:d}\".format(i+1,N_runs), end=\"\\r\")\n",
    "    acc = trainer.validate()[-1]\n",
    "    print('accuracies: ', acc)\n",
    "    accuracies.append(acc)\n",
    "    reinitialize(trainer.model)\n",
    "\n",
    "# Evaluate average test accuracy\n",
    "print('\\n===========')\n",
    "print('No analog errors during training, CrossSim analog errors during test')\n",
    "accuracy_digitalTrain_analogTest = np.mean(accuracies)\n",
    "std_digitalTrain_analogTest = np.std(accuracies)\n",
    "print('Test accuracy: {:.2f}% +/- {:.3f}%'.format(100*accuracy_digitalTrain_analogTest, 100*std_digitalTrain_analogTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.3039 | Val Loss: 0.4566 | Val Acc: 0.8754\n",
      "Saving best model with accuracy: 0.8754\n",
      "Epoch [2/100] Train Loss: 0.2875 | Val Loss: 0.3882 | Val Acc: 0.8757\n",
      "Saving best model with accuracy: 0.8757\n",
      "Epoch [3/100] Train Loss: 0.2772 | Val Loss: 0.2044 | Val Acc: 0.9361\n",
      "Saving best model with accuracy: 0.9361\n",
      "Epoch [4/100] Train Loss: 0.2695 | Val Loss: 0.2546 | Val Acc: 0.9238\n",
      "Epoch [5/100] Train Loss: 0.2566 | Val Loss: 0.2194 | Val Acc: 0.9310\n",
      "Epoch [6/100] Train Loss: 0.2555 | Val Loss: 0.1610 | Val Acc: 0.9529\n",
      "Saving best model with accuracy: 0.9529\n",
      "Epoch [7/100] Train Loss: 0.2526 | Val Loss: 0.3400 | Val Acc: 0.8971\n",
      "Epoch [8/100] Train Loss: 0.2614 | Val Loss: 0.3441 | Val Acc: 0.8945\n",
      "Epoch [9/100] Train Loss: 0.2497 | Val Loss: 0.1934 | Val Acc: 0.9436\n",
      "Epoch [10/100] Train Loss: 0.2564 | Val Loss: 0.3001 | Val Acc: 0.9142\n",
      "Epoch [11/100] Train Loss: 0.2507 | Val Loss: 0.2660 | Val Acc: 0.9210\n",
      "Epoch [12/100] Train Loss: 0.2434 | Val Loss: 0.4793 | Val Acc: 0.8503\n",
      "Epoch [13/100] Train Loss: 0.2497 | Val Loss: 0.2249 | Val Acc: 0.9313\n",
      "Epoch [14/100] Train Loss: 0.2334 | Val Loss: 0.3153 | Val Acc: 0.9098\n",
      "Epoch [15/100] Train Loss: 0.2431 | Val Loss: 0.4243 | Val Acc: 0.8730\n",
      "Epoch [16/100] Train Loss: 0.2429 | Val Loss: 0.2814 | Val Acc: 0.9158\n",
      "Epoch [17/100] Train Loss: 0.2327 | Val Loss: 0.2636 | Val Acc: 0.9173\n",
      "Epoch [18/100] Train Loss: 0.2431 | Val Loss: 0.2335 | Val Acc: 0.9327\n",
      "Epoch [19/100] Train Loss: 0.2313 | Val Loss: 0.3122 | Val Acc: 0.9060\n",
      "Epoch [20/100] Train Loss: 0.2319 | Val Loss: 0.3579 | Val Acc: 0.8831\n",
      "Epoch [21/100] Train Loss: 0.2305 | Val Loss: 0.2681 | Val Acc: 0.9181\n",
      "Epoch [22/100] Train Loss: 0.2297 | Val Loss: 0.4282 | Val Acc: 0.8784\n",
      "Epoch [23/100] Train Loss: 0.2210 | Val Loss: 0.3505 | Val Acc: 0.8909\n",
      "Epoch [24/100] Train Loss: 0.2218 | Val Loss: 0.2566 | Val Acc: 0.9249\n",
      "Epoch [25/100] Train Loss: 0.2343 | Val Loss: 0.1849 | Val Acc: 0.9466\n",
      "Epoch [26/100] Train Loss: 0.2331 | Val Loss: 0.2424 | Val Acc: 0.9275\n",
      "Epoch [27/100] Train Loss: 0.2364 | Val Loss: 0.2411 | Val Acc: 0.9260\n",
      "Epoch [28/100] Train Loss: 0.2228 | Val Loss: 0.2430 | Val Acc: 0.9227\n",
      "Epoch [29/100] Train Loss: 0.2169 | Val Loss: 0.1337 | Val Acc: 0.9587\n",
      "Saving best model with accuracy: 0.9587\n",
      "Epoch [30/100] Train Loss: 0.2277 | Val Loss: 0.2055 | Val Acc: 0.9391\n",
      "Epoch [31/100] Train Loss: 0.2256 | Val Loss: 0.2994 | Val Acc: 0.9013\n",
      "Epoch [32/100] Train Loss: 0.2248 | Val Loss: 0.3105 | Val Acc: 0.9002\n",
      "Epoch [33/100] Train Loss: 0.2258 | Val Loss: 0.1691 | Val Acc: 0.9457\n",
      "Epoch [34/100] Train Loss: 0.2326 | Val Loss: 0.4007 | Val Acc: 0.8767\n",
      "Epoch [35/100] Train Loss: 0.2263 | Val Loss: 0.1681 | Val Acc: 0.9491\n",
      "Epoch [36/100] Train Loss: 0.2250 | Val Loss: 0.2754 | Val Acc: 0.9161\n",
      "Epoch [37/100] Train Loss: 0.2204 | Val Loss: 0.3350 | Val Acc: 0.8963\n",
      "Epoch [38/100] Train Loss: 0.2235 | Val Loss: 0.1366 | Val Acc: 0.9559\n",
      "Epoch [39/100] Train Loss: 0.2338 | Val Loss: 0.4428 | Val Acc: 0.8648\n",
      "Epoch [40/100] Train Loss: 0.2185 | Val Loss: 0.2005 | Val Acc: 0.9301\n",
      "Epoch [41/100] Train Loss: 0.2292 | Val Loss: 0.2431 | Val Acc: 0.9253\n",
      "Epoch [42/100] Train Loss: 0.2157 | Val Loss: 0.2791 | Val Acc: 0.9081\n",
      "Epoch [43/100] Train Loss: 0.2216 | Val Loss: 0.2161 | Val Acc: 0.9327\n",
      "Epoch [44/100] Train Loss: 0.2122 | Val Loss: 0.2101 | Val Acc: 0.9405\n",
      "Epoch [45/100] Train Loss: 0.2203 | Val Loss: 0.1947 | Val Acc: 0.9414\n",
      "Epoch [46/100] Train Loss: 0.2210 | Val Loss: 0.1453 | Val Acc: 0.9547\n",
      "Epoch [47/100] Train Loss: 0.2232 | Val Loss: 0.1528 | Val Acc: 0.9558\n",
      "Epoch [48/100] Train Loss: 0.2311 | Val Loss: 0.1567 | Val Acc: 0.9528\n",
      "Epoch [49/100] Train Loss: 0.2180 | Val Loss: 0.1583 | Val Acc: 0.9531\n",
      "Epoch [50/100] Train Loss: 0.2239 | Val Loss: 0.1845 | Val Acc: 0.9423\n",
      "Epoch [51/100] Train Loss: 0.2084 | Val Loss: 0.2533 | Val Acc: 0.9170\n",
      "Epoch [52/100] Train Loss: 0.2189 | Val Loss: 0.2177 | Val Acc: 0.9330\n",
      "Epoch [53/100] Train Loss: 0.2240 | Val Loss: 0.1231 | Val Acc: 0.9651\n",
      "Saving best model with accuracy: 0.9651\n",
      "Epoch [54/100] Train Loss: 0.2250 | Val Loss: 0.2148 | Val Acc: 0.9363\n",
      "Epoch [55/100] Train Loss: 0.2145 | Val Loss: 0.3892 | Val Acc: 0.8857\n",
      "Epoch [56/100] Train Loss: 0.2189 | Val Loss: 0.1807 | Val Acc: 0.9463\n",
      "Epoch [57/100] Train Loss: 0.2173 | Val Loss: 0.2136 | Val Acc: 0.9345\n",
      "Epoch [58/100] Train Loss: 0.2213 | Val Loss: 0.3264 | Val Acc: 0.9060\n",
      "Epoch [59/100] Train Loss: 0.2261 | Val Loss: 0.5508 | Val Acc: 0.8394\n",
      "Epoch [60/100] Train Loss: 0.2251 | Val Loss: 0.2923 | Val Acc: 0.9092\n",
      "Epoch [61/100] Train Loss: 0.2203 | Val Loss: 0.1868 | Val Acc: 0.9417\n",
      "Epoch [62/100] Train Loss: 0.2130 | Val Loss: 0.5575 | Val Acc: 0.8522\n",
      "Epoch [63/100] Train Loss: 0.2260 | Val Loss: 0.4309 | Val Acc: 0.8931\n",
      "Epoch [64/100] Train Loss: 0.2131 | Val Loss: 0.1678 | Val Acc: 0.9477\n",
      "Epoch [65/100] Train Loss: 0.2156 | Val Loss: 0.2808 | Val Acc: 0.9133\n",
      "Epoch [66/100] Train Loss: 0.2169 | Val Loss: 0.2743 | Val Acc: 0.9114\n",
      "Epoch [67/100] Train Loss: 0.2230 | Val Loss: 0.2052 | Val Acc: 0.9363\n",
      "Epoch [68/100] Train Loss: 0.2031 | Val Loss: 0.2402 | Val Acc: 0.9264\n",
      "Epoch [69/100] Train Loss: 0.2099 | Val Loss: 0.2988 | Val Acc: 0.9110\n",
      "Epoch [70/100] Train Loss: 0.2136 | Val Loss: 0.2351 | Val Acc: 0.9268\n",
      "Epoch [71/100] Train Loss: 0.2218 | Val Loss: 0.1814 | Val Acc: 0.9463\n",
      "Epoch [72/100] Train Loss: 0.2203 | Val Loss: 0.3443 | Val Acc: 0.8961\n",
      "Epoch [73/100] Train Loss: 0.2161 | Val Loss: 0.1961 | Val Acc: 0.9385\n",
      "Epoch [74/100] Train Loss: 0.2143 | Val Loss: 0.1963 | Val Acc: 0.9414\n",
      "Epoch [75/100] Train Loss: 0.2132 | Val Loss: 0.2110 | Val Acc: 0.9367\n",
      "Epoch [76/100] Train Loss: 0.2162 | Val Loss: 0.3539 | Val Acc: 0.8977\n",
      "Epoch [77/100] Train Loss: 0.2179 | Val Loss: 0.1793 | Val Acc: 0.9442\n",
      "Epoch [78/100] Train Loss: 0.2242 | Val Loss: 0.2140 | Val Acc: 0.9336\n",
      "Epoch [79/100] Train Loss: 0.2162 | Val Loss: 0.1703 | Val Acc: 0.9477\n",
      "Epoch [80/100] Train Loss: 0.2181 | Val Loss: 0.2241 | Val Acc: 0.9324\n",
      "Epoch [81/100] Train Loss: 0.2129 | Val Loss: 0.2908 | Val Acc: 0.9170\n",
      "Epoch [82/100] Train Loss: 0.2160 | Val Loss: 0.2674 | Val Acc: 0.9171\n",
      "Epoch [83/100] Train Loss: 0.2150 | Val Loss: 0.1742 | Val Acc: 0.9467\n",
      "Epoch [84/100] Train Loss: 0.2150 | Val Loss: 0.1272 | Val Acc: 0.9603\n",
      "Epoch [85/100] Train Loss: 0.2160 | Val Loss: 0.1989 | Val Acc: 0.9399\n",
      "Epoch [86/100] Train Loss: 0.2182 | Val Loss: 0.2040 | Val Acc: 0.9424\n",
      "Epoch [87/100] Train Loss: 0.2112 | Val Loss: 0.3394 | Val Acc: 0.9037\n",
      "Epoch [88/100] Train Loss: 0.2176 | Val Loss: 0.1497 | Val Acc: 0.9522\n",
      "Epoch [89/100] Train Loss: 0.2044 | Val Loss: 0.1935 | Val Acc: 0.9452\n",
      "Epoch [90/100] Train Loss: 0.2115 | Val Loss: 0.2016 | Val Acc: 0.9363\n",
      "Epoch [91/100] Train Loss: 0.2197 | Val Loss: 0.2127 | Val Acc: 0.9370\n",
      "Epoch [92/100] Train Loss: 0.2199 | Val Loss: 0.3195 | Val Acc: 0.9147\n",
      "Epoch [93/100] Train Loss: 0.2137 | Val Loss: 0.2013 | Val Acc: 0.9393\n",
      "Epoch [94/100] Train Loss: 0.2179 | Val Loss: 0.2210 | Val Acc: 0.9318\n",
      "Epoch [95/100] Train Loss: 0.2118 | Val Loss: 0.1871 | Val Acc: 0.9463\n",
      "Epoch [96/100] Train Loss: 0.2071 | Val Loss: 0.2522 | Val Acc: 0.9304\n",
      "Epoch [97/100] Train Loss: 0.2083 | Val Loss: 0.2401 | Val Acc: 0.9304\n",
      "Epoch [98/100] Train Loss: 0.2158 | Val Loss: 0.2819 | Val Acc: 0.9220\n",
      "Epoch [99/100] Train Loss: 0.2200 | Val Loss: 0.2682 | Val Acc: 0.9180\n",
      "Epoch [100/100] Train Loss: 0.2095 | Val Loss: 0.1918 | Val Acc: 0.9421\n",
      "Training complete. Best validation accuracy: 0.9651\n"
     ]
    }
   ],
   "source": [
    "from applications.mvm_params import set_params\n",
    "from simulator.algorithms.dnn.torch.convert import from_torch\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "model = LeNet5Adapter(num_classes=10, rank=1)\n",
    "model.load_state_dict(torch.load('./trained_models/basline_mnist_instanceNorm.pth', weights_only=False), strict=False)\n",
    "params_analog = set_params(weight_bits = 8, wtmodel = \"BALANCED\", \n",
    "                         error_model = \"generic\",\n",
    "                         proportional_error = False,\n",
    "                         alpha_error = 0.1,\n",
    "                         useGPU = True,\n",
    "                         gpu_id = gpu_id)\n",
    "\n",
    "model = from_torch(model, params_analog)\n",
    "\n",
    "params = []\n",
    "\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (LoRA2d, LoRALinear, nn.BatchNorm1d, nn.BatchNorm2d, nn.InstanceNorm2d, nn.LayerNorm)):\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad_(True)\n",
    "            params.append(p)\n",
    "    else:\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad_(True)\n",
    "\n",
    "gpu_id = 0\n",
    "model_path = './trained_models/adapter_alpha0.1_rank1_mnist_instanceNorm.pth'\n",
    "device = torch.device('cuda:'+str(gpu_id) if torch.cuda.is_available() else 'cpu')\n",
    "optim = torch.optim.SGD(params, lr=0.0001, momentum=0.9, weight_decay=5e-5)\n",
    "trainer = Trainer(model=model, optimizer=optim,\n",
    "                  num_epochs=100, scheduler=None, train_dataset=ds_train, val_dataset=ds_val, batch_size=128, \n",
    "                  device=device, criterion=nn.CrossEntropyLoss(), best_model_path=model_path)\n",
    "trainer.train(crossSim=True, adapter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Accuracy:  0.98725\n",
      "accuracies:  0.9186666666666666\n",
      "accuracies:  0.9344166666666667\n",
      "accuracies:  0.9536666666666667\n",
      "accuracies:  0.9539166666666666\n",
      "accuracies:  0.9505833333333333\n",
      "accuracies:  0.9059166666666667\n",
      "accuracies:  0.9195833333333333\n",
      "accuracies:  0.9515833333333333\n",
      "accuracies:  0.9396666666666667\n",
      "accuracies:  0.9233333333333333\n",
      "accuracies:  0.94975 11 of 100\n",
      "accuracies:  0.9314166666666667\n",
      "accuracies:  0.94375 13 of 100\n",
      "accuracies:  0.9479166666666666\n",
      "accuracies:  0.9269166666666667\n",
      "accuracies:  0.9086666666666666\n",
      "accuracies:  0.8881666666666667\n",
      "accuracies:  0.9025833333333333\n",
      "accuracies:  0.9531666666666667\n",
      "accuracies:  0.9608333333333333\n",
      "accuracies:  0.9529166666666666\n",
      "accuracies:  0.9385833333333333\n",
      "accuracies:  0.94975 23 of 100\n",
      "accuracies:  0.9440833333333334\n",
      "accuracies:  0.9464166666666667\n",
      "accuracies:  0.9343333333333333\n",
      "accuracies:  0.93425 27 of 100\n",
      "accuracies:  0.9485n 28 of 100\n",
      "accuracies:  0.9063333333333333\n",
      "accuracies:  0.9329166666666666\n",
      "accuracies:  0.93425 31 of 100\n",
      "accuracies:  0.8910833333333333\n",
      "accuracies:  0.9340833333333334\n",
      "accuracies:  0.9301666666666667\n",
      "accuracies:  0.9444166666666667\n",
      "accuracies:  0.9184166666666667\n",
      "accuracies:  0.93325 37 of 100\n",
      "accuracies:  0.9278333333333333\n",
      "accuracies:  0.8990833333333333\n",
      "accuracies:  0.8945833333333333\n",
      "accuracies:  0.9285n 41 of 100\n",
      "accuracies:  0.9415833333333333\n",
      "accuracies:  0.8858333333333334\n",
      "accuracies:  0.9465n 44 of 100\n",
      "accuracies:  0.9470833333333334\n",
      "accuracies:  0.9265833333333333\n",
      "accuracies:  0.9169166666666667\n",
      "accuracies:  0.89825 48 of 100\n",
      "accuracies:  0.8736666666666667\n",
      "accuracies:  0.9345833333333333\n",
      "accuracies:  0.83275 51 of 100\n",
      "accuracies:  0.9603333333333334\n",
      "accuracies:  0.9436666666666667\n",
      "accuracies:  0.9420833333333334\n",
      "accuracies:  0.9010833333333333\n",
      "accuracies:  0.8816666666666667\n",
      "accuracies:  0.9224166666666667\n",
      "accuracies:  0.9229166666666667\n",
      "accuracies:  0.9455833333333333\n",
      "accuracies:  0.90575 60 of 100\n",
      "accuracies:  0.90875 61 of 100\n",
      "accuracies:  0.9166666666666666\n",
      "accuracies:  0.9236666666666666\n",
      "accuracies:  0.9423333333333334\n",
      "accuracies:  0.9351666666666667\n",
      "accuracies:  0.9518333333333333\n",
      "accuracies:  0.927on 67 of 100\n",
      "accuracies:  0.94775 68 of 100\n",
      "accuracies:  0.9503333333333334\n",
      "accuracies:  0.9103333333333333\n",
      "accuracies:  0.94225 71 of 100\n",
      "accuracies:  0.9261666666666667\n",
      "accuracies:  0.9240833333333334\n",
      "accuracies:  0.9475833333333333\n",
      "accuracies:  0.9425833333333333\n",
      "accuracies:  0.9465833333333333\n",
      "accuracies:  0.9535833333333333\n",
      "accuracies:  0.9131666666666667\n",
      "accuracies:  0.8806666666666667\n",
      "accuracies:  0.9345833333333333\n",
      "accuracies:  0.9069166666666667\n",
      "accuracies:  0.944on 82 of 100\n",
      "accuracies:  0.93675 83 of 100\n",
      "accuracies:  0.9589166666666666\n",
      "accuracies:  0.9490833333333333\n",
      "accuracies:  0.9145833333333333\n",
      "accuracies:  0.923on 87 of 100\n",
      "accuracies:  0.9175n 88 of 100\n",
      "accuracies:  0.9505833333333333\n",
      "accuracies:  0.9370833333333334\n",
      "accuracies:  0.9035833333333333\n",
      "accuracies:  0.91775 92 of 100\n",
      "accuracies:  0.9510833333333333\n",
      "accuracies:  0.8624166666666667\n",
      "accuracies:  0.9456666666666667\n",
      "accuracies:  0.9460833333333334\n",
      "accuracies:  0.929on 97 of 100\n",
      "accuracies:  0.9538333333333333\n",
      "accuracies:  0.9016666666666666\n",
      "accuracies:  0.9418333333333333\n",
      "\n",
      "===========\n",
      "No analog errors during training, CrossSim analog errors during test\n",
      "Test accuracy: 92.83% +/- 2.322%\n"
     ]
    }
   ],
   "source": [
    "from applications.mvm_params import set_params\n",
    "from simulator.algorithms.dnn.torch.convert import from_torch, reinitialize\n",
    "torch.use_deterministic_algorithms(False)\n",
    "gpu_id = 0\n",
    "device = torch.device('cuda:'+str(gpu_id) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a parameters object that models a memory device with very large errors\n",
    "params_analog = set_params(weight_bits = 8, wtmodel = \"BALANCED\", \n",
    "                         error_model = \"generic\",\n",
    "                         proportional_error = False,\n",
    "                         alpha_error = 0.1, useGPU = True, gpu_id = gpu_id, set_seed=0)\n",
    "\n",
    "# Convert the layers in the trained CNN\n",
    "model = LeNet5Adapter(num_classes=10, rank=1)\n",
    "# if not model_path:\n",
    "model_path = './trained_models/adapter_alpha0.1_rank1_mnist_instanceNorm.pth'\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, weights_only=False), strict=True)\n",
    "trainer.model = model.to(device)\n",
    "trainer.device = device\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "print('Loaded Accuracy: ', trainer.validate()[-1])\n",
    "analog_mnist_cnn_pt = from_torch(model.to(device), params_analog)\n",
    "trainer.model = analog_mnist_cnn_pt\n",
    "\n",
    "# Number of inference simulations with re-sampled random analog errors\n",
    "N_runs = 100\n",
    "\n",
    "set_seed(2024)\n",
    "\n",
    "# Perform analog inference on the test set\n",
    "accuracies = []\n",
    "trainer.model.eval()\n",
    "\n",
    "for i in range(N_runs):\n",
    "    print(\"Inference simulation {:d} of {:d}\".format(i+1,N_runs), end=\"\\r\")\n",
    "    acc = trainer.validate(recalibrate=False)[-1]\n",
    "    print('accuracies: ', acc)\n",
    "    accuracies.append(acc)\n",
    "    reinitialize(trainer.model)\n",
    "\n",
    "# Evaluate average test accuracy\n",
    "print('\\n===========')\n",
    "print('No analog errors during training, CrossSim analog errors during test')\n",
    "accuracy_digitalTrain_analogTest = np.mean(accuracies)\n",
    "std_digitalTrain_analogTest = np.std(accuracies)\n",
    "print('Test accuracy: {:.2f}% +/- {:.3f}%'.format(100*accuracy_digitalTrain_analogTest, 100*std_digitalTrain_analogTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super(RMSBatchNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        # Running statistics\n",
    "        self.register_buffer('running_rms', torch.ones(num_features))\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "        \n",
    "        # Learnable parameters (affine transformation)\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.ones(num_features))  # Scale\n",
    "            self.beta = nn.Parameter(torch.zeros(num_features))  # Shift\n",
    "        else:\n",
    "            self.gamma = None\n",
    "            self.beta = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape: (batch_size, num_features, height, width)\n",
    "        if self.training:\n",
    "            # Compute mean and per-channel RMS over (N, H, W)\n",
    "            mean = x.mean(dim=[0, 2, 3], keepdim=True)  # Per-channel mean\n",
    "            rms = torch.sqrt(torch.mean((x - mean) ** 2, dim=[0, 2, 3], keepdim=True) + self.eps)  # RMS after mean subtraction\n",
    "\n",
    "            # Update running stats\n",
    "            if self.track_running_stats:\n",
    "                with torch.no_grad():\n",
    "                    self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.squeeze()\n",
    "                    self.running_rms = (1 - self.momentum) * self.running_rms + self.momentum * rms.squeeze()\n",
    "\n",
    "        else:  # In eval mode, use stored running statistics\n",
    "            mean = self.running_mean.view(1, self.num_features, 1, 1)\n",
    "            rms = self.running_rms.view(1, self.num_features, 1, 1)\n",
    "\n",
    "        # Normalize input\n",
    "        x_norm = (x - mean) / rms  \n",
    "\n",
    "        # Apply affine transformation (scale and shift)\n",
    "        if self.affine:\n",
    "            x_norm = x_norm * self.gamma.view(1, -1, 1, 1) + self.beta.view(1, -1, 1, 1)\n",
    "\n",
    "        return x_norm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSBatchNorm1d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super(RMSBatchNorm1d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "        \n",
    "        # Running statistics (mean and RMS)\n",
    "        self.register_buffer('running_rms', torch.ones(num_features))\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "\n",
    "        # Learnable parameters (affine transformation)\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.ones(num_features))  # Scale (like BatchNorm weight)\n",
    "            self.beta = nn.Parameter(torch.zeros(num_features))  # Shift (like BatchNorm bias)\n",
    "        else:\n",
    "            self.gamma = None\n",
    "            self.beta = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expected shape: (batch_size, num_features) OR (batch_size, num_features, sequence_length)\n",
    "        if x.dim() == 2:  # Convert (batch, num_features) to (batch, num_features, 1) if needed\n",
    "            x = x.unsqueeze(2)\n",
    "\n",
    "        if self.training:\n",
    "            # Compute mean per feature\n",
    "            mean = x.mean(dim=[0, 2], keepdim=True)  # Per-feature mean\n",
    "            rms = torch.sqrt(torch.mean((x - mean) ** 2, dim=[0, 2], keepdim=True) + self.eps)  # RMS after mean subtraction\n",
    "\n",
    "            # Update running stats\n",
    "            if self.track_running_stats:\n",
    "                with torch.no_grad():\n",
    "                    self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.squeeze()\n",
    "                    self.running_rms = (1 - self.momentum) * self.running_rms + self.momentum * rms.squeeze()\n",
    "\n",
    "        else:  # In eval mode, use stored running mean & running RMS\n",
    "            mean = self.running_mean.view(1, self.num_features, 1)\n",
    "            rms = self.running_rms.view(1, self.num_features, 1)\n",
    "\n",
    "        # Normalize input\n",
    "        x_norm = (x - mean) / rms\n",
    "\n",
    "        # Apply affine transformation (scale and shift)\n",
    "        if self.affine:\n",
    "            x_norm = x_norm * self.gamma.view(1, -1, 1) + self.beta.view(1, -1, 1)\n",
    "\n",
    "        return x_norm.squeeze(2)  # Remove extra dimension if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MaxAbsBatchNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalization layer that zero-centers the data and scales by the maximum absolute deviation.\n",
    "    \n",
    "    For an input x, it computes:\n",
    "    \n",
    "        mu    = mean(x)           over (N, H, W) for each channel\n",
    "        s     = x - mu\n",
    "        scale = max(|s|)            over (N, H, W) for each channel\n",
    "        x_out = (x - mu) / (scale + eps)\n",
    "    \n",
    "    Optionally, an affine transformation is applied:\n",
    "    \n",
    "        y = gamma * x_out + beta\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, \n",
    "                 affine=True, track_running_stats=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): number of channels in the input.\n",
    "            eps (float): a value added to the denominator for numerical stability.\n",
    "            momentum (float): momentum for the running estimates.\n",
    "            affine (bool): if True, this module has learnable affine parameters.\n",
    "            track_running_stats (bool): if True, this module tracks running mean and scale.\n",
    "        \"\"\"\n",
    "        super(MaxAbsBatchNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            # running_mean and running_scale have shape (C,)\n",
    "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "            self.register_buffer('running_scale', torch.ones(num_features))\n",
    "        if self.affine:\n",
    "            # Learnable scale and shift parameters (per channel)\n",
    "            self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "            self.beta  = nn.Parameter(torch.zeros(num_features))\n",
    "        else:\n",
    "            self.gamma = None\n",
    "            self.beta  = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (N, C, H, W)\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # Compute the per-channel mean over batch, height, and width.\n",
    "            mean = x.mean(dim=[0, 2, 3], keepdim=True)  # shape: (1, C, 1, 1)\n",
    "            centered = x - mean  # zero-centered data\n",
    "\n",
    "            # Compute the maximum absolute deviation over (N, H, W) for each channel.\n",
    "            # One way to do this is to reshape all (N, H, W) elements into one dimension per channel.\n",
    "            # First, permute to (C, N, H, W) then flatten to (C, -1):\n",
    "            scale = centered.abs().permute(1, 0, 2, 3).contiguous().view(self.num_features, -1).max(dim=1)[0]\n",
    "            scale = scale.view(1, self.num_features, 1, 1)\n",
    "            # Avoid division by zero\n",
    "            scale = scale + self.eps\n",
    "\n",
    "            # Update running statistics, if desired.\n",
    "            if self.track_running_stats:\n",
    "                with torch.no_grad():\n",
    "                    running_mean = x.mean(dim=[0, 2, 3])  # shape: (C,)\n",
    "                    running_scale = centered.abs().permute(1, 0, 2, 3).contiguous().view(self.num_features, -1).max(dim=1)[0]\n",
    "                    self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * running_mean\n",
    "                    self.running_scale = (1 - self.momentum) * self.running_scale + self.momentum * running_scale\n",
    "        else:\n",
    "            # In eval mode, use the stored running estimates.\n",
    "            mean = self.running_mean.view(1, self.num_features, 1, 1)\n",
    "            scale = self.running_scale.view(1, self.num_features, 1, 1) + self.eps\n",
    "\n",
    "        # Normalize: subtract the mean and divide by the maximum absolute deviation.\n",
    "        x_norm = (x - mean) / scale\n",
    "\n",
    "        # Optionally apply the learnable affine transformation.\n",
    "        if self.affine:\n",
    "            x_norm = x_norm * self.gamma.view(1, self.num_features, 1, 1) + self.beta.view(1, self.num_features, 1, 1)\n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MaxAbsBatchNorm2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalization layer that:\n",
    "      - Zero-centers the input (subtracts the mean computed over (N, H, W) for each channel).\n",
    "      - Scales the centered data by dividing by the maximum absolute deviation \n",
    "        (i.e. max(|x - mean|) computed over (N, H, W) for each channel) plus a small epsilon.\n",
    "      \n",
    "    Optionally applies an affine transformation:\n",
    "        y = gamma * normalized_x + beta\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, \n",
    "                 affine=True, track_running_stats=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): Number of channels.\n",
    "            eps (float): Small constant added to the denominator for numerical stability.\n",
    "            momentum (float): Momentum for the running statistics update.\n",
    "            affine (bool): If True, learnable parameters gamma (scale) and beta (shift) are used.\n",
    "            track_running_stats (bool): If True, running estimates for mean and scale are maintained.\n",
    "        \"\"\"\n",
    "        super(MaxAbsBatchNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "\n",
    "        # Initialize running statistics if tracking is enabled.\n",
    "        if self.track_running_stats:\n",
    "            # These buffers will not be considered parameters.\n",
    "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "            self.register_buffer('running_scale', torch.ones(num_features))\n",
    "        # Initialize learnable affine parameters if required.\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.ones(num_features))\n",
    "            self.beta  = nn.Parameter(torch.zeros(num_features))\n",
    "        else:\n",
    "            self.gamma = None\n",
    "            self.beta  = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (N, C, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Normalized tensor with the same shape as x.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            # Compute per-channel mean over (N, H, W)\n",
    "            mean = x.mean(dim=[0, 2, 3], keepdim=True)  # shape: (1, C, 1, 1)\n",
    "            centered = x - mean  # Zero-centered data\n",
    "\n",
    "            # Efficiently compute the maximum absolute deviation:\n",
    "            # Instead of a permute+view combination, we transpose to get shape (C, N, H, W)\n",
    "            # then reshape to (C, N*H*W) and take the max along the last dimension.\n",
    "            centered_abs = centered.abs().transpose(0, 1).reshape(self.num_features, -1)\n",
    "            scale_val = centered_abs.max(dim=1)[0]  # shape: (C,)\n",
    "            scale = scale_val.view(1, self.num_features, 1, 1) + self.eps  # add eps for safety\n",
    "\n",
    "            # Update running statistics (in-place for efficiency).\n",
    "            if self.track_running_stats:\n",
    "                with torch.no_grad():\n",
    "                    # Remove extra dimensions for running stats (shape: (C,))\n",
    "                    current_mean = mean.squeeze()          # shape: (C,)\n",
    "                    # Update using in-place mul_ and add_\n",
    "                    self.running_mean.mul_(1 - self.momentum).add_(self.momentum * current_mean)\n",
    "                    self.running_scale.mul_(1 - self.momentum).add_(self.momentum * scale_val)\n",
    "        else:\n",
    "            # In evaluation mode, use the stored running statistics.\n",
    "            mean = self.running_mean.view(1, self.num_features, 1, 1)\n",
    "            scale = self.running_scale.view(1, self.num_features, 1, 1) + self.eps\n",
    "\n",
    "        # Normalize the input.\n",
    "        x_norm = (x - mean) / scale\n",
    "\n",
    "        # Apply affine transformation if enabled.\n",
    "        if self.affine:\n",
    "            x_norm = x_norm * self.gamma.view(1, self.num_features, 1, 1) + self.beta.view(1, self.num_features, 1, 1)\n",
    "        return x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGxCAYAAACwbLZkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVshJREFUeJzt3XtcVHX+P/DXwAwDIgz3m1wEMlRQUyjFLTVJvKRd1v1mmym21a7di/XnRlZqbaHmb39mXy/Veqm11C20dDXLLdFKTFE0S9QsFBAQQbmIym0+vz9wjjMwIDNczsyZ1/PxmIczh3Nm3odh4OXndlRCCAEiIiIimTjJXQARERE5NoYRIiIikhXDCBEREcmKYYSIiIhkxTBCREREsmIYISIiIlkxjBAREZGsGEaIiIhIVgwjREREJCuGEepya9euhUqlQnZ2ttmvT5w4Eb179zbZ1rt3b8yYMcOi19m7dy/mzZuHiooK6wp1QBs3bkRsbCzc3NygUqlw+PBhs/tlZmZCpVJBpVIhKyurxddnzJiBnj17dnG1XWfevHlQqVTt3s9w69GjB0JDQzF27Fi88847qK6ubnHMjBkzWvx830hRURHmzZvX6vvRGnOvpVKp8PTTT1v0PDeyfPlyrF27tsX206dPQ6VSmf0aUVsYRsgmbd68Ga+88opFx+zduxfz589nGGmn8+fPY9q0aYiOjsaOHTuQlZWFm2+++YbHzZ49uxuqs22G79eOHTuwePFihIeHY/bs2YiNjcWRI0dM9n3llVewefNmi56/qKgI8+fPtziMWPNa1mgtjAQHByMrKwt33313l9dAyqKWuwAicwYPHix3CRarr6+HSqWCWm0fH6uTJ0+ivr4eDz/8MEaOHNmuY8aNG4cdO3Zg69atmDRpUpfVdvnyZfTo0aPLnr+j4uPj4efnJz1+8MEH8fTTT2PkyJG45557cPLkSWi1WgBAdHR0l9dj+H51x2u1RavVYtiwYbLWQPaJLSNkk5p30+j1evz9739HTEwM3Nzc4OXlhYEDB+Ltt98G0NR8/n/+z/8BAERGRkrN6JmZmdLxixYtQt++faHVahEQEIDp06ejsLDQ5HWFEHjzzTcREREBV1dXJCQkYOfOnRg1ahRGjRol7WfotvjXv/6Fv/71r+jVqxe0Wi1OnTqF8+fP48knn0T//v3Rs2dPBAQEYPTo0fj2229NXsvQpP3WW29h4cKF6N27N9zc3DBq1CgpKLz44osICQmBTqfD/fffj9LS0nZ9/7Zs2YLExET06NEDHh4eGDNmjEn3yowZM3D77bcDAKZMmQKVSmVyfq2ZMWMG+vfvj7S0NDQ2Nra5b3u/56NGjUJcXBz27NmD4cOHo0ePHvjTn/7UKd+fjRs3Ijk5GcHBwXBzc0O/fv3w4osvoqampl3fR0sMGjQIc+bMQX5+PjZu3ChtN9d18sknn2Do0KHQ6XTo0aMHoqKi8Kc//QlA08/WrbfeCgB45JFHpJ/lefPmSc/Xs2dPHD16FMnJyfDw8EBSUlKrr2Xw7rvv4uabb4ZWq0X//v2xYcMGk6+31lVl6GY9ffo0gKbP5s8//4zdu3dLtRles7Vumu+++w5JSUnw8PBAjx49MHz4cGzbts3s6+zatQtPPPEE/Pz84Ovri9///vcoKioye06kHAwj1G0aGxvR0NDQ4taeC0cvWrQI8+bNwx//+Eds27YNGzduxKOPPip1yTz22GN45plnAACbNm1CVlYWsrKyMGTIEADAE088gb/97W8YM2YMtmzZgtdffx07duzA8OHDUVZWJr3OnDlzMGfOHIwbNw6ff/45Zs6cicceewwnT540W1daWhry8/OxcuVKbN26FQEBAbhw4QIAYO7cudi2bRvWrFmDqKgojBo1SgpHxpYtW4bvv/8ey5Ytwz//+U8cP34ckyZNwqOPPorz589j9erVWLRoEf773//iscceu+H36uOPP8a9994LT09PrF+/HqtWrcLFixcxatQofPfddwCamvOXLVsGAHjzzTeRlZWF5cuX3/C5nZ2dkZ6ejp9//hkffPBBm/u293sOAMXFxXj44Yfx0EMPYfv27XjyySc75fvzyy+/YMKECVi1ahV27NiB559/Hv/+97+7rFXnnnvuAQDs2bOn1X2ysrIwZcoUREVFYcOGDdi2bRteffVVNDQ0AACGDBmCNWvWAABefvll6WfZ+Nzq6upwzz33YPTo0fj8888xf/78NuvasmULli5ditdeew2ffvopIiIi8Mc//hGffvqpxee4efNmREVFYfDgwVJtbXUN7d69G6NHj0ZlZSVWrVqF9evXw8PDA5MmTTIJbQaPPfYYNBoNPv74YyxatAiZmZl4+OGHLa6T7Iwg6mJr1qwRANq8RUREmBwTEREhUlJSpMcTJ04Ut9xyS5uv89ZbbwkAIi8vz2R7bm6uACCefPJJk+0//PCDACBeeuklIYQQFy5cEFqtVkyZMsVkv6ysLAFAjBw5Utq2a9cuAUCMGDHihuff0NAg6uvrRVJSkrj//vul7Xl5eQKAGDRokGhsbJS2L1myRAAQ99xzj8nzPP/88wKAqKysbPW1GhsbRUhIiBgwYIDJc1ZXV4uAgAAxfPjwFufwySef3PAcmu97++23i9DQUHHlyhUhhBApKSnC3d1d2r+933MhhBg5cqQAIL7++muTfTv7+6PX60V9fb3YvXu3ACCOHDkifW3u3LmiPb8ODfudP3/e7NevXLkiAIjx48dL21JSUkx+vhcvXiwAiIqKilZf58CBAwKAWLNmTYuvpaSkCABi9erVZr/W/LMEQLi5uYmSkhJpW0NDg+jbt6+46aabWpxbc4bPr/HnKjY21uTzYGB4z4zrHjZsmAgICBDV1dUmrx8XFydCQ0OFXq83eZ3mPzOLFi0SAERxcXGL1yPlYMsIdZsPP/wQBw4caHEzdBe05bbbbsORI0fw5JNP4ssvv0RVVVW7X3fXrl0A0GJ2zm233YZ+/frh66+/BgDs27cPtbW1eOCBB0z2GzZsWKtN35MnTza7feXKlRgyZAhcXV2hVquh0Wjw9ddfIzc3t8W+EyZMgJPT9Y9iv379AKDFIEDD9vz8/FbOFDhx4gSKioowbdo0k+fs2bMnJk+ejH379uHy5cutHt9eCxcuRGFhodRN1lx7v+cG3t7eGD16tNnn6sj357fffsNDDz2EoKAgODs7Q6PRSONjzL0XHSXa0cpn6IJ54IEH8O9//xtnz5616rVa+9kzJykpCYGBgdJjZ2dnTJkyBadOnWrRbdaZampq8MMPP+APf/iDyWwrZ2dnTJs2DYWFhThx4oTJMYbWJYOBAwcCAM6cOdNldZL8GEao2/Tr1w8JCQktbjqd7obHpqWlYfHixdi3bx/Gjx8PX19fJCUltTpd2Fh5eTmAppH+zYWEhEhfN/xr/EvbwNy21p7zH//4B5544gkMHToUGRkZ2LdvHw4cOIBx48bhypUrLfb38fExeezi4tLm9qtXr5qtxfgcWjtXvV6Pixcvtnp8ew0fPhz33XcfFixYYPb52vs9NzC3n4G1359Lly7hjjvuwA8//IC///3vyMzMxIEDB7Bp0yYAMPtedJThD2ZISEir+4wYMQKfffYZGhoaMH36dISGhiIuLg7r169v9+v06NEDnp6e7d4/KCio1W3N34vOdPHiRQghWv05MPf6vr6+Jo8NA4G74v0i28EwQnZBrVYjNTUVhw4dwoULF7B+/XoUFBRg7NixN/yfvuGXW3FxcYuvFRUVSbMiDPudO3euxX4lJSVmn9vcgL9169Zh1KhRWLFiBe6++24MHToUCQkJZteg6Gw3OlcnJyd4e3t3ymulp6ejuroab775psV1GM9EAcx/Hzvqm2++QVFREVavXo3HHnsMI0aMQEJCAjw8PDr9tQy2bNkCADccDHzvvffi66+/RmVlJTIzMxEaGoqHHnrI7Bou5lj6/TL382vYZnivXF1dAQC1tbUm+zUf32MJb29vODk5tfpzAKDFzwI5JoYRsjteXl74wx/+gKeeegoXLlyQRvm39j8oQ/P/unXrTLYfOHAAubm50kyEoUOHQqvVthhUt2/fPouaiFUqlVSLwY8//tjuPzQdERMTg169euHjjz826TKoqalBRkaGNMOmM/Tt2xd/+tOf8M4777ToOmrv97wrGf5gN38v3n333S55vSNHjuDNN99E7969W3T1tUar1WLkyJFYuHAhACAnJ0faDnRea8DXX39tErIbGxuxceNGREdHIzQ0FACkrsgff/zR5NitW7earbs9tbm7u2Po0KHYtGmTyf56vR7r1q1DaGhou9a2IeWzjwURyOFNmjQJcXFxSEhIgL+/P86cOYMlS5YgIiICffr0AQAMGDAAAPD2228jJSUFGo0GMTExiImJwZ///Ge88847cHJywvjx43H69Gm88sorCAsLwwsvvACgqdk/NTUV6enp8Pb2xv3334/CwkLMnz8fwcHBJuMW2jJx4kS8/vrrmDt3LkaOHIkTJ07gtddeQ2RkpDRjoqs4OTlh0aJFmDp1KiZOnIi//OUvqK2txVtvvYWKigosWLCgU19v3rx5+Oijj7Br1y64u7tL29v7Pe9Kw4cPh7e3N2bOnIm5c+dCo9Hgo48+arEomTUOHjwInU6H+vp6FBUV4euvv8a//vUvBAQEYOvWrVKXkTmvvvoqCgsLkZSUhNDQUFRUVODtt982Gc8SHR0NNzc3fPTRR+jXrx969uyJkJCQNrt/2uLn54fRo0fjlVdegbu7O5YvX47jx4+bTO+dMGECfHx88Oijj+K1116DWq3G2rVrUVBQ0OL5BgwYgA0bNmDjxo2IioqCq6ur9PlrLj09HWPGjMGdd96JWbNmwcXFBcuXL8dPP/2E9evXd0mrGNkfhhGyC3feeScyMjLwz3/+E1VVVQgKCsKYMWPwyiuvQKPRAGhqGk9LS8MHH3yA999/H3q9Hrt27ZK6TKKjo7Fq1SosW7YMOp0O48aNQ3p6ukkf9RtvvAF3d3esXLkSa9asQd++fbFixQrMmTMHXl5e7ap1zpw5uHz5MlatWoVFixahf//+WLlyJTZv3mx2am9ne+ihh+Du7o709HRMmTIFzs7OGDZsGHbt2oXhw4d36muFhITg+eefN9tV097veVfx9fXFtm3b8Ne//hUPP/ww3N3dce+992Ljxo3SlG9rjRs3DkBTC4GPjw8GDBiAhQsX4pFHHrlhN9DQoUORnZ2Nv/3tbzh//jy8vLyQkJCAb775BrGxsQCaxoSsXr0a8+fPR3JyMurr6zF37lxprRFL3XPPPYiNjcXLL7+M/Px8REdH46OPPsKUKVOkfTw9PaXpzw8//DC8vLzw2GOPYfz48S2mTM+fPx/FxcV4/PHHUV1djYiICKmFsrmRI0fim2++wdy5czFjxgzo9XoMGjQIW7ZswcSJE606H1IelWjP8G8iB5aXl4e+ffti7ty5eOmll+Quh4hIcRhGiIwcOXIE69evx/Dhw+Hp6YkTJ05g0aJFqKqqwk8//dTqrBoiIrIeu2mIjLi7uyM7OxurVq1CRUUFdDodRo0ahTfeeINBhIioi7BlhIiIiGTFqb1EREQkK4YRIiIikhXDCBEREcnKLgaw6vV6FBUVwcPDgwvkEBER2QkhBKqrqxESEtLmwpF2EUaKiooQFhYmdxlERERkhYKCAunSA+bYRRgxrGhYUFBg0ZUqiYiISD5VVVUICwu74crEdhFGDF0znp6eDCNERER25kZDLDiAlYiIiGTFMEJERESyYhghIiIiWTGMEBERkawYRoiIiEhWDCNEREQkK4YRIiIikhXDCBEREcmKYYSIiIhk1aEwkp6eDpVKheeff77N/Xbv3o34+Hi4uroiKioKK1eu7MjLEhERkYJYHUYOHDiA9957DwMHDmxzv7y8PEyYMAF33HEHcnJy8NJLL+HZZ59FRkaGtS9NRERECmJVGLl06RKmTp2K999/H97e3m3uu3LlSoSHh2PJkiXo168fHnvsMfzpT3/C4sWLrSqYiIiIlMWqMPLUU0/h7rvvxl133XXDfbOyspCcnGyybezYscjOzkZ9fb3ZY2pra1FVVWVyIyLbVFtbC71eL3cZRGTHLA4jGzZswKFDh5Cent6u/UtKShAYGGiyLTAwEA0NDSgrKzN7THp6OnQ6nXQLCwuztEwi6gaZmZkICgpCnz59UFJSInc5RGSnLAojBQUFeO6557Bu3Tq4urq2+7jmlw4WQpjdbpCWlobKykrpVlBQYEmZRNQNioqKMGXKFFRUVOC3337Da6+9JndJRGSnLAojBw8eRGlpKeLj46FWq6FWq7F7924sXboUarUajY2NLY4JCgpq8T+m0tJSqNVq+Pr6mn0drVYLT09PkxsR2Y6GhgY8+OCDKC0tlba9//77yMvLk7EqIrJXFoWRpKQkHD16FIcPH5ZuCQkJmDp1Kg4fPgxnZ+cWxyQmJmLnzp0m27766iskJCRAo9F0rHoiksXLL7+Mb7/9FgDg5NT0a6ShoQHz58+XsywislMWhREPDw/ExcWZ3Nzd3eHr64u4uDgATV0s06dPl46ZOXMmzpw5g9TUVOTm5mL16tVYtWoVZs2a1blnQkTd4j//+Q8WLlwIAFCr1di2bZs0q+5f//oXcnNz5SyPiOxQp6/AWlxcjPz8fOlxZGQktm/fjszMTNxyyy14/fXXsXTpUkyePLmzX5qIutjp06dN/rOxaNEijBs3DrNnzwYA6PV6vPrqq3KVR0R2SiUMo0ltWFVVFXQ6HSorKzl+hEgmQgjccccd+P777wEA999/PzIyMqBSqVBTU4Po6GicO3cOAHDo0CEMHjxYznKJyAa09+83r01DRO1y+vRpKYhERkZi9erV0ow4d3d3zJkzR9r35ZdflqVGIrJParkLICL7sHfvXul+SkoKvLy8TL7+5z//GYsXL0Z+fj62b9+OvXv3Yvjw4S2eZ+uJrS22TYqZ1On1EpH9YMsIEbWLoVUEgNmQodVqTcaLvP76691SFxHZP4YRImoXQ8uIk5MThg4danaflJQU9OrVCwCwZ88eNDQ0dFt9RGS/GEaIqFVbT2zF1hNbsfHgRhw9ehQAMGDAgFYHoqnVavzud78DAFy+fBnHjx/vtlqJyH4xjBDRDZ04ckK6GJ65LhpjCQkJ0v3s7OwurYuIlIFhhIhu6HjO9RYOQ8tHa4zDyIEDB7qsJiJSDoYRIroh4zByo5aRIUOGSPfZMkJE7cEwQkRtamxsxPHDTWHE298bvXv3bnN/nU6Hm2++GQBw5MgR1NXVdXWJRGTnGEaIqE35p/JxpeYKAKDf4H7SQmdtMXTV1NbW4ueff+7S+ojI/jGMEFGbjLto+g7u265jOIiViCzBMEJEbco9dP0qvP2G9GvXMQwjRGQJLgdPRG0ytIxoXDSI6hfVrmMGDx4MlUoFIQS+/u5rs0vAExEZsGWEiFp18fxFlBSUAAD6DOgDjYumXcf17NkT/fo1taKc+eUM6mo5iJWIWscwQkStys2xvIvGwNBV01DfgDMnz3RqXUSkLAwjRNQqawavGhiPG/nlp186rSYiUh6GESJqlUkYucX6MHLqp1OdVhMRKQ/DCBGZdfXqVZz6uSlE9OrdCzofnUXHDxo0CE7OTb9iGEaIqC0MI0Rk1sGDB9FQ3wDA8i4aAOjRowfCbwoH0LRwWu2V2k6tj4iUg2GEiMzau3evdN+aMAIAN8XdBADQN+qRdzyvU+oiIuVhGCEisw4fPizdjxkUY9VzGMIIwK4aImodwwgRmfXLL00zYFQqFUJ6h1j1HH3i+lx/Ps6oIaJWMIwQUQtCCCmM+AX7wUXrYtXz9I7pDbWmaaFnw2BYIqLmGEaIqIXy8nJUVFQAAEIirGsVAZqWkI/oEwEAKPy1ULr6LxGRMYYRImrB0CoCAMERwR16LsO4ESEEfj32a4eei4iUiWGEiFowDiMdaRkBgOjYaOn+6eOnO/RcRKRMDCNE1EJnhpHQyFDpfnFBcYeei4iUiWGEiFowCSNWzqQxCAoPku6X5Jd06LmISJkYRoioBUMYcXJyQmBoYIeeyyfABxoXDQCgpIBhhIhaYhghIhPG03r9Q/ylIGEtJycnBIU1tY6UFJRAr9d3uEYiUhaGESIyUVpaiurqagAdHy9iYAgj9XX1uFB6oVOek4iUg2GEiEx05rReA0MYAThuhIhaYhghIhOdOXjVwDiMcEYNETXHMEJEJk6dur5se6d10xjNqDlXcK5TnpOIlMOiMLJixQoMHDgQnp6e8PT0RGJiIr744otW98/MzIRKpWpxO378eIcLJ6KuYdJNE96ym2bria1mb20xfp7ifLaMEJEptSU7h4aGYsGCBbjppqblnT/44APce++9yMnJQWxsbKvHnThxAp6entJjf39/K8sloq5mCCPOzs4dntZrEBgaCJVKBSEEp/cSUQsWhZFJkyaZPH7jjTewYsUK7Nu3r80wEhAQAC8vr3a/Tm1tLWpra6XHVVVVlpRJRFYyntbbu/f1K+52lMZFA98gX5QVl3EAKxG1YPWYkcbGRmzYsAE1NTVITExsc9/BgwcjODgYSUlJ2LVr1w2fOz09HTqdTrqFhYVZWyYRWaCkpAQ1NTUAgD59+nTqcweHNXXVVFdW41LVpU59biKybxaHkaNHj6Jnz57QarWYOXMmNm/ejP79+5vdNzg4GO+99x4yMjKwadMmxMTEICkpCXv27GnzNdLS0lBZWSndCgoKLC2TiKxgPF6ks8MIl4UnotZY3AYbExODw4cPo6KiAhkZGUhJScHu3bvNBpKYmBjExMRIjxMTE1FQUIDFixdjxIgRrb6GVquFVqu1tDQi6qAuDSPGa40UlOCmuJs69fmJyH5Z3DLi4uKCm266CQkJCUhPT8egQYPw9ttvt/v4YcOGmfzCIyLb0ZVhhDNqiKg1HV5nRAhhMtj0RnJychAc3DmrOhJR59l6Yiv2HLrehZrvnN+pz9+8ZYSIyMCibpqXXnoJ48ePR1hYGKqrq7FhwwZkZmZix44dAJrGepw9exYffvghAGDJkiXo3bs3YmNjUVdXh3Xr1iEjIwMZGRmdfyZE1GFFZ4oAAM5qZwSEBHTqc3PMCBG1xqIwcu7cOUybNg3FxcXQ6XQYOHAgduzYgTFjxgAAiouLkZ9//X9TdXV1mDVrFs6ePQs3NzfExsZi27ZtmDBhQueeBRF1mF6vl7pPgkKD4Kx27tTn7+nZEx46D1RXVnNJeCIyYVEYWbVqVZtfX7t2rcnj2bNnY/bs2RYXRUTd70LpBdRdrQPQeRfIay4oPAjVR6tRXlKO+rp6aFw0XfI6RGRfeG0aIgIAFJ0uku531gXymjOMGxFC4Fwhr1FDRE0YRogIwPXxIkDnXSCvOQ5iJSJzGEaICABQfOb6OA5zF8jrDBzESkTmMIwQEQDTlpHg3l0TRgxLwgPgIFYikjCMEBGA6wuRqTVq+Ad3zZW12TJCROYwjBAR9Hq9FA6CwoLg7Ny503oNfAJ8pBk0HDNCRAYMI0SEwsJC1NU2TevtqsGrAODk5CQNYi0pKIFer++y1yIi+2HxhfKISHnOnDkj3Q8MDbTqObae2Nqu/YLCglDwawHq6+pxofQC/IL8rHo9IlIOtowQEQoKCqT7fsFdGw44boSImmMYISKTyzj4h3TN4FUDzqghouYYRojItGWki7tN2DJCRM0xjBCRactIF03rNeAqrETUHMMIEUktI85qZ3j5eXXpawWGBkKlUgG4vrYJETk2hhEiklpGfAN9u2yNEQONi0YaJHuugBfLIyKGESKHV1NTg4sXLwLo+pk0BgEhAQCA6spqXL18tVtek4hsF8MIkYMzHrza1eNFDIwHyZaVlHXLaxKR7WIYIXJwxoNXu2sBMuMWGIYRImIYIXJwJi0jXbzGiIFJy0gxwwiRo2MYIXJwJtN6g7onjPgG+Ur32TJCRAwjRA7OZMGzkO7ppjEOPQwjRMQwQuTg5GgZ4ZgRIjLGMELk4AwtI2493ODu6d4tr+np7QmNiwYAwwgRMYwQOTQhhNQy4hfsJ62M2tVUKpU0boQDWImIYYTIgZWXl+Pq1aZFx7prwTMDQ5fQ5UuXUVVV1a2vTUS2hWGEyIF15wXymjMOP4WFhd362kRkW9RyF0BE8jGZSdPNLSO+gden936y9xMMcR7SYp9JMZO6syQikglbRogcmBwzaQw4o4aIDBhGiByYHGuMGHCtESIyYBghcmBsGSEiW8AwQuTAjFtGjJdo7w7GY0Y4vZfIsTGMEDkwQxjx9/eH1lXbra/t4eUBF1cXAED5ufJufW0isi0MI0QOqqGhAWfPngUAhIeHd/vrq1QqqWuorLgMQohur4GIbAPDCJGDKi4uhl6vBwCEhYXJUoOha+jK5Suoqa6RpQYikp9FYWTFihUYOHAgPD094enpicTERHzxxRdtHrN7927Ex8fD1dUVUVFRWLlyZYcKJqLOYTx4VY6WEQDwCzIaxMpxI0QOy6IwEhoaigULFiA7OxvZ2dkYPXo07r33Xvz8889m98/Ly8OECRNwxx13ICcnBy+99BKeffZZZGRkdErxRGQ948GrcrWMGM+o4bgRIsdl0QqskyaZrob4xhtvYMWKFdi3bx9iY2Nb7L9y5UqEh4djyZIlAIB+/fohOzsbixcvxuTJk62vmog6zNZaRs4Xn5elBiKSn9VjRhobG7FhwwbU1NQgMTHR7D5ZWVlITk422TZ27FhkZ2ejvr6+1eeura1FVVWVyY2IOpdNtIwEca0RIrIijBw9ehQ9e/aEVqvFzJkzsXnzZvTv39/sviUlJQgMDDTZFhgYiIaGBpSVtf6LJz09HTqdTrrJ9YuSSMlsrWWkvITdNESOyuIwEhMTg8OHD2Pfvn144oknkJKSgmPHjrW6v0qlMnlsmL7XfLuxtLQ0VFZWSjfj/8ERUecwfK7UajWCgoJkqYGrsBIRYMVVe11cXHDTTTcBABISEnDgwAG8/fbbePfdd1vsGxQUhJKSEpNtpaWlUKvV8PVtfbVHrVYLrbZ7F2AicjSGlpFevXrB2dlZlhrcPdzh2sMVVy9f5WwaIgfW4XVGhBCora01+7XExETs3LnTZNtXX32FhIQEaDSajr40EVnp8uXLKC9v6haRq4sGaGohNXTVnC85z4XPiByURWHkpZdewrfffovTp0/j6NGjmDNnDjIzMzF16lQATd0r06dPl/afOXMmzpw5g9TUVOTm5mL16tVYtWoVZs2a1blnQUQWsYXBqwaGMFJ3tQ6XKi/JWgsRycOibppz585h2rRpKC4uhk6nw8CBA7Fjxw6MGTMGQNOKjsaD4iIjI7F9+3a88MILWLZsGUJCQrB06VJO6yWSmXEYkbNlBGg5bsTDy0PGaohIDhaFkVWrVrX59bVr17bYNnLkSBw6dMiiooioaxn/p0H2lpFA01VYI/tGylgNEcmB16YhckA22zJyjoNYiRwRwwiRA7LFMSMAr09D5KgYRogckM2GEa41QuSQGEaIHNDZs2cBAK6urvD29pa1FpNuGraMEDkkhhEiB2QII7169WpzNeTu0KNnD/To2QMAx4wQOSqGESIHc/nyZVRUVABoCiO2wNA6Ul5SzoXPiBwQwwiRgzG0igC2E0Z8A5suD1FXW4eqi7xKN5GjYRghcjC2GEb8g/2l+xzESuR4GEaIHIwthhHOqCFybAwjRA7GOIyEhITIWMl1XGuEyLExjBA5GFtsGfEN8pXus2WEyPEwjBA5mKKiIum+zYSRwOth5ELpBRkrISI5MIwQORhb7KZhGCFybAwjRA7GEEb8/f3h4uIiczVNevTsAdcergCA8nPlMldDRN2NYYTIgej1eqmbxla6aABApVLBx98HAMMIkSNiGCFyIOfPn0dDQwMA2wojwPWumis1V3D50mWZqyGi7sQwQuRAbHG8iIFPoI90n+NGiBwLwwiRA7HFab0GPgHXw0h5KbtqiByJWu4CiKjrbD2x1eTxF4e+kO7bWhgxmVFzji0jRI6ELSNEDsR4cKjNhZEATu8lclQMI0QOxKbDiFHLCGfUEDkWhhEiB2Lc4mBrYcRkzAjDCJFD4ZgRIgdiGIuhcdHg+/PfQ1Wmkrmi67z9vaX7HMBK5FjYMkLkQAx/5H0CfKBS2U4QAZoCks5XB4BjRogcDcMIkYOovVqLS5WXAJiOz7Alhq6ai+cvQq/Xy1wNEXUXhhEiB2E8DsNWw4hhRk1jQyMqyytlroaIugvDCJGDMF67w3iwqC3h1XuJHBPDCJGDsIuWEU7vJXJIDCNEDsJ4hoqthhEuCU/kmBhGiBwEW0aIyFYxjBA5COM/7rY6ZoRX7iVyTAwjRA7C+I+7rYYR4+vTsGWEyHEwjBA5CMNsGg8vD7hoXWSuxjwPLw9oXDQAeOVeIkdiURhJT0/HrbfeCg8PDwQEBOC+++7DiRMn2jwmMzMTKpWqxe348eMdKpyI2k+v1+PC+aY/7rY6XgQAVCqV1GrDbhoix2FRGNm9ezeeeuop7Nu3Dzt37kRDQwOSk5NRU1Nzw2NPnDiB4uJi6danTx+riyYiy1RdrEJDfQMA2w4jwPX6qiurceXKFZmrIaLuYNGF8nbs2GHyeM2aNQgICMDBgwcxYsSINo8NCAiAl5eXxQUSUcfZw0waA+PxLEVFRYiOjpaxGiLqDh0aM1JZ2bRcs4/PjQfDDR48GMHBwUhKSsKuXbva3Le2thZVVVUmNyKynj2svmpgPIi1qKhIxkqIqLtYHUaEEEhNTcXtt9+OuLi4VvcLDg7Ge++9h4yMDGzatAkxMTFISkrCnj17Wj0mPT0dOp1OuoWFhVlbJhHBPqb1GhhP7z179qyMlRBRd7Gom8bY008/jR9//BHfffddm/vFxMQgJiZGepyYmIiCggIsXry41a6dtLQ0pKamSo+rqqoYSIg6wB5WXzUwDksMI0SOwaqWkWeeeQZbtmzBrl27EBoaavHxw4YNwy+//NLq17VaLTw9PU1uRGQ9exozYlwfwwiRY7CoZUQIgWeeeQabN29GZmYmIiMjrXrRnJwcBAcHW3UsEVnOXsMIx4wQOQaLwshTTz2Fjz/+GJ9//jk8PDxQUlICANDpdHBzcwPQ1MVy9uxZfPjhhwCAJUuWoHfv3oiNjUVdXR3WrVuHjIwMZGRkdPKpEFFrDGt2qDVqeHrbdksju2mIHI9FYWTFihUAgFGjRplsX7NmDWbMmAEAKC4uRn5+vvS1uro6zJo1C2fPnoWbmxtiY2Oxbds2TJgwoWOVE1G7GWbT+AT4QKVSyVxN27SuWvTU9cSlyksMI0QOwuJumhtZu3atyePZs2dj9uzZFhVFRJ2n9motqiurAdh+F42Bb4AvLlVeQlFREYQQNh+giKhjeG0aIoUzXlbdXsKIYXpvbW0tLlzgsvBESscwQqRw9rTGiAHHjRA5FoYRIoUzmUkTYB8tI5zeS+RYGEaIFM5kKfhA+2gZ4fReIsfCMEKkcPa0+qqBcQsOW0aIlI9hhEjh7GnBMwOOGSFyLAwjRApnjwNY2U1D5FgYRogUzjBmxEPnAa2rVuZq2kfnq4OTc9OvJ7aMECkfwwiRggkhcOH8tdVX7WTwKgA4OTnBx7+pXoYRIuVjGCFSsKqLVWiobwBgP+NFDAzhqbS0FHV1dTJXQ0RdiWGESMHscfCqgfGMmuLiYhkrIaKuxjBCpGD2uOCZARc+I3IcDCNECmYyk8aOxowADCNEjoRhhEjBjFdftbtumiCGESJHwTBCpGD2uPqqAVdhJXIcDCNECmbXA1iN6i0sLJSxEiLqagwjRApmCCNqjRqe3p4yV2MZjhkhchwMI0QKZhgz4hPgA5VKJXM1ltG6aeHl5QWAYYRI6RhGiBTqypUrqK6sBmB/XTQGoaGhAJrCiBBC5mqIqKswjBAplPEF5uw1jPTq1QsAUFtbiwsXLtxgbyKyVwwjRApl3LVhbwueGRjCCMBBrERKxjBCpFDGYcTeFjwzMA4jHDdCpFwMI0QKpYRumnLn61OTtx/cjq0ntspYDRF1FYYRIoUyaRkJsM+WEeMQZbxmChEpC8MIkUKZjBmx05YR4yXhGUaIlIthhEih2DJCRPaCYYRIoQxhxEPnAa2rVuZqrOPp7Qm1Rg2AYYRIyRhGiBRICCENYLXXmTQAoFKppFYd4ysQE5GyMIwQKVBZWRnq6uoA2O94EQND/dWV1ai9WitzNUTUFdRyF0BEncN42utvub9J9+09jPgF+Un32VVDpExsGSFSIOM/2va6+qqB8eBbdtUQKRPDCJECGYcRex4zAnBGDZEjYBghUiDjFgR776YxCSOlDCNESmRRGElPT8ett94KDw8PBAQE4L777sOJEydueNzu3bsRHx8PV1dXREVFYeXKlVYXTEQ3ZvxHW1FhhC0jRIpkURjZvXs3nnrqKezbtw87d+5EQ0MDkpOTUVNT0+oxeXl5mDBhAu644w7k5OTgpZdewrPPPouMjIwOF09E5pmMGVFSGClhGCFSIotm0+zYscPk8Zo1axAQEICDBw9ixIgRZo9ZuXIlwsPDsWTJEgBAv379kJ2djcWLF2Py5MnWVU1EbbpQ2tRNo9ao4entKXM1HWM8gJUtI0TK1KExI5WVlQAAH5/WB8hlZWUhOTnZZNvYsWORnZ2N+vp6s8fU1taiqqrK5EZE7Wf4o+0T4AOVSiVzNR2jcdFA56MDwDEjREpldRgRQiA1NRW333474uLiWt2vpKQEgYGBJtsCAwPR0NCAsrIys8ekp6dDp9NJt7CwMGvLJHI4dbV1qK6oBmD/XTQGhhlBF89fhF6vl7kaIupsVoeRp59+Gj/++CPWr19/w32b/89MCGF2u0FaWhoqKyulW0FBgbVlEjkcQxcNoJwwYlgrpbGhEaWlpTJXQ0SdzaoVWJ955hls2bIFe/bsQWhoaJv7BgUFoaSkxGRbaWkp1Go1fH3N/6LUarXQau3zwl5EclPSgmcGxquwFhYWIigoSMZqiKizWdQyIoTA008/jU2bNuGbb75BZGTkDY9JTEzEzp07TbZ99dVXSEhIgEajsaxaIrohJS14ZmB8HoarERORclgURp566imsW7cOH3/8MTw8PFBSUoKSkhJcuXJF2ictLQ3Tp0+XHs+cORNnzpxBamoqcnNzsXr1aqxatQqzZs3qvLMgIomSpvUaGJ8HwwiR8lgURlasWIHKykqMGjUKwcHB0m3jxo3SPsXFxcjPz5ceR0ZGYvv27cjMzMQtt9yC119/HUuXLuW0XqIuoqTVVw2Mu5sYRoiUx6IxI4aBp21Zu3Zti20jR47EoUOHLHkpIrKSklZfNWDLCJGy8do0RApjMmYkQBljRnyDroeRwsJCGSshoq7AMEKkMIYl0z28POCidZG5ms7h7uEOrVvTDDu2jBApD8MIkYI0NjZK3TT+If4yV9N5VCqVNG6EYYRIeRhGiBSkoqwCjQ2NAEzX5lACw7iR6upqXiKCSGEYRogU5Hzxeem+0sII1xohUi6GESIFKSu5fr0n/2DldNMApuGKYYRIWRhGiBSkrPh6GFFaywjXGiFSLoYRIgUxbhnxC1ZWGGE3DZFyMYwQKYhxy4jSummMFz7jWiNEysIwQqQghgGsKpVKMQueGTS/ci8RKQfDCJGCGLppvP29odZYdLUHm2d8TsbXvyIi+8cwQqQQ9XX1qCirAKC88SIA4OTkJLX2MIwQKQvDCJFClJeWSxezVNpMGgPDOJiLFy+iurpa5mqIqLMwjBAphMng1SBlDV41MF7ivqCgQMZKiKgzMYwQKYSSp/UaGM8QYhghUg6GESKFOF90fSl4pU3rNTA+L44bIVIOhhEihXCIlpEQhhEiJWIYIVIIJS8Fb8CWESJlYhghUghDy4hao4aXn5e8xXQR4xYfhhEi5WAYIVIIQ8uIT4APnJyU+dHu0bMHdDodAIYRIiVR5m8sIgdTU1OD6sqmdTeUOl7EIDw8HEDTbBq9Xi9zNUTUGRhGiBTAeJqrUmfSGBjCSH19PUpLS2Wuhog6A8MIkQIYhxGlDl41MIQRgF01RErBMEKkAI7YMgIwjBApBcMIkQKwZYSI7BnDCJECmIQRhQ9gDQsLk+4zjBApA8MIkQIY/1FmywgR2RuGESIFMLSMuLi6wMPLQ+ZqulZISIi0jgrDCJEyMIwQ2TkhhBRG/IP9oVKpZK6oa2k0GoSEhADglXuJlIJhhMjOVVRUoKamBoDyx4sYGLpqSktLceXKFZmrIaKOYhghsnOONJPGwHjcSGFhoYyVEFFnYBghsnPG4yb8g5S9xogBB7ESKQvDCJGdc6RpvQac3kukLBaHkT179mDSpEkICQmBSqXCZ5991ub+mZmZUKlULW7Hjx+3tmYiMuLo3TQMI0T2T23pATU1NRg0aBAeeeQRTJ48ud3HnThxAp6entJjf3/HaE4m6momS8GHOMbnimGESFksDiPjx4/H+PHjLX6hgIAAeHl5WXwcEbXN0VtGCg4dArZubf/BkyZ1QUVE1BEWhxFrDR48GFevXkX//v3x8ssv484772x139raWtTW1kqPq6qquqNEIrtkCCPunu5wc3dr8fXAb/Zb9bznRt/Wobq6kre3N9zd3VFTU4P88+flLoeIOqjLB7AGBwfjvffeQ0ZGBjZt2oSYmBgkJSVhz549rR6Tnp4OnU4n3YwHqxHRdXq9XgojjtIqsvXEVvzn5H/gE+QDADhdeg5CCJmrIqKO6PKWkZiYGMTExEiPExMTUVBQgMWLF2PEiBFmj0lLS0Nqaqr0uKqqioGEyIzS0lLU19cDaFp91ZH4Bfuh4NcC1NY3oLy6Gn5GY9KIyL50WzeNsWHDhmHdunWtfl2r1UKr1XZjRUT2SSnjRazpSgoXAjnX7uefP88wQmTHZFlnJCcnB8HBwXK8NJGiOOIaIwaB3jrpPseNENk3i1tGLl26hFOnTkmP8/LycPjwYfj4+CA8PBxpaWk4e/YsPvzwQwDAkiVL0Lt3b8TGxqKurg7r1q1DRkYGMjIyOu8siBzU6dOnpfuOMq3XIMjneksIwwiRfbM4jGRnZ5vMhDGM7UhJScHatWtRXFxsMu+/rq4Os2bNwtmzZ+Hm5obY2Fhs27YNEyZM6ITyiRzbb7/9Jt0PCg2SsZLuZ9wyUlBWJmMlRNRRFoeRUaNGtTlyfe3atSaPZ8+ejdmzZ1tcGBHdmEkYCXOsMMKWESLl4LVpiOxYXl4eAMDNzQ1efl7yFtPNArwYRoiUgmGEyE7p9XopjERFRUGlUslcUfdy0ajh69kTAMMIkb1jGCGyU8XFxdJKxZGRkTJXI49A76bWkeKLF1F3bb0VIrI/DCNEdsp4vEhUVJSMlcgn8Nq4ESEECsvLZa6GiKwly6JnRNRxXR1GrFmIrLuvZxPs4yXdP33uHKKCHGsQL5FSsGWEyE4ZxosAjtsy0sto0O6vJSXyFUJEHcIwQmSnjFtGHHXMSKi/t3SfYYTIfrGbhshebN1q8vC37GzpfmRuLi6X/djdFcmul59RGCkulrESIuoItowQ2anfzp0DAAR6ecHd1VXmauQR7KuDs1PTlGa2jBDZL4YRIjt0pbYWxRcuAIBDD9pUOztLy8L/WlLS5urQRGS7GEaI7NDp0lLpfmRgoIyVyM8wbqTq8mWUV1fLXA0RWYNjRojs0G9GXRJRNhRGrJkO3FFN40aaZhb9WlwMP0/Ptg8gIpvDlhEiO2QYLwI4djcNAPTijBoiu8cwQmSHTFpGHDyMhHJGDZHdYxghskN5Ri0jjj5mhC0jRPaPYYTIDhlaRjRqNXr5+Mhcjby4CiuR/WMYIbIzQghpzEjvgAA4OzvLXJG83F21CNBdn95LRPaHYYTIzpyvrETN1asAbGsmjZyig4MBAMUXLuByba3M1RCRpTi1l8jO5HEmTQvRQUHIOn4cQFMXVlxEROs7N1tWv90mTbLuOCK6IbaMENkZ45k0jj541SDaKJRxRg2R/WEYIbIzXGOkJUM3DWD6/SEi+8AwQmRnbHX1VTmxZYTIvjGMENkZjhlpyfj7wBk1RPaHYYTIzhi6Ibx79oTO3V3mamxDoJcX3F1dATCMENkjhhEiO1JXX4+CsjIAbBUxplKppC6r06WlaGxslLkiIrIEwwiRHck/fx56vR4Ax4s0ZxjEWt/QIAU2IrIPXGeEyI4YzxRxcxfYX7hfxmpsS3SzcSO9GdaI7AZbRojsiPHg1WBfnYyV2B7OqCGyXwwjRHbEeFpvLz/vNvZ0PMZrjXAQK5F9YRghsiOmYcRLvkJsUPNuGiKyHwwjRHbEMGbESaVCkA+7aYyF+/vD2anpVxq7aYjsC8MIkZ0QQkj/4w/09oTa2VnmimyLRq1GREAAgKaWESGEzBURUXtxNg1RR3TjFWBLLl5EZU0NACAiyNe611W46KAg/FZSguorV1BWVQV/HVuPiOyBxS0je/bswaRJkxASEgKVSoXPPvvshsfs3r0b8fHxcHV1RVRUFFauXGlNrUQO7VhBgXQ/MshfxkpsF2fUENkni1tGampqMGjQIDzyyCOYPHnyDffPy8vDhAkT8Pjjj2PdunX4/vvv8eSTT8Lf379dxxMpkhUtKrlGYaQ3W0bMaj6jZljfvjJWQ0TtZXEYGT9+PMaPH9/u/VeuXInw8HAsWbIEANCvXz9kZ2dj8eLFDCNEFjBpGQlmy4g5nFFDZJ+6fABrVlYWkpOTTbaNHTsW2dnZqK+vN3tMbW0tqqqqTG5Eju6YScuIn4yV2C7jMPIbwwiR3ejyMFJSUoLAZssyBwYGoqGhAWWtXD8iPT0dOp1OuoWFhXV1mUQ2z9BN4+PpDp27m8zV2CbjbpoTZ8/KWAkRWaJbpvaqVCqTx4Ypd823G6SlpaGyslK6FRj9j5DIEZVXVaG0shIA0DuQrSKtcXd1lab3Hiso4PReIjvR5VN7g4KCUNKsubS0tBRqtRq+vuYH4Wm1Wmi12q4ujchu5BYWSvcjgxlG2hIbHo4zpaWounwZhWVlCPPn+BoiW9flLSOJiYnYuXOnybavvvoKCQkJ0Gg0Xf3yRIpwLD9fuh/J8SJtigsPl+7/ZPR9IyLbZXEYuXTpEg4fPozDhw8DaJq6e/jwYeRf+9CnpaVh+vTp0v4zZ87EmTNnkJqaitzcXKxevRqrVq3CrFmzOucMiBwAW0baL9YojPzMMEJkFyzupsnOzsadd94pPU5NTQUApKSkYO3atSguLpaCCQBERkZi+/bteOGFF7Bs2TKEhIRg6dKlnNZLZAHOpGm/uIgI6f5PZ87IWAkRtZfFYWTUqFFtDgpbu3Zti20jR47EoUOHLH0pIrrGMJPGy90dvp49Za7GtvUNDYVKpYIQgi0jRHaC16YhsmH7C/ej5motCq5Ngw8L8G51Fpoj21+43+RxLz8vFJ6/iGMFBdDr9XBy4jVBiWwZP6FENu5MSbl0n+NF2ifq2gq1l2trcbq0VOZqiOhGGEaIbFxeyXnpPseLtE90yPXpvBw3QmT7GEaIbNxptoxYzPjaPRw3QmT7GEaIbFxeMVtGLBUdEiDd51ojRLaPYYTIxuWVNA1edXXRIMhbJ3M19iE8wAfOTk0DfdkyQmT7GEaIbNjVunoUlVUAACICfeHkxJk07eGiUSMsoOlyE7kFBWhobJS5IiJqC6f2Etmw/NIL0F9b14fjRSwTFeyP0yVlqGtowK/FxYgJDe3YE27dat1xkyZ17HWJHABbRohs2OlrXTQAEBnEC75ZwmRGDbtqiGwawwiRDTMevMoL5FmGM2qI7Ae7aYgA65vguxin9VqPa40Q2Q+2jBDZMMOCZxq1M0L8vGWuxr6E+vvARd30/y22jBDZNoYRIhtV39CAgtILAJqmqqqd+XG1hNrZCX2vDVo9WVSEuvp6mSsiotbwtxuRjfq1pAQNjXoAXOzMWrHh4QCAhsZGnCwqkrkaImoNwwiRjcotKJDuc/CqdeKuhRGAXTVEtoxhhMhGHTUadMmWEevEGoURDmIlsl0MI0Q26sAvv0j3Y8KDZazEfsVFREj32TJCZLsYRohskBAC+0+eBAB4uLkizJ8zaawRGRgINxcXAFz4jMiWMYwQ2aD88+dRWlkJAOjfOwQqFa9JYw0nJyf0v9ZV82tJCa7U1spcERGZwzBCZIMMrSJAUxgh68WGhQEA9Ho9cgsLZa6GiMxhGCGyQfuNxovERjCMdMTA3r2l+wdPnZKvECJqFcMIkQ0yaRlhGOmQoTEx0v2s48dlrISIWsMwQmRjGhobkX3tf/BBPp7w1fWUuSL7Fh8dDbWzMwAg68QJmashInMYRohsTG5BAS5fG2jZP6KXzNXYPzetFkOiowEAxwsLcaG6WuaKiKg5hhEiG8PBq50v0air5gej7y8R2QaGESIbYzx4NY5hpFMk9u0r3ee4ESLbwzBCZGMMLSNOTk6ICePKq53BOIzsZRghsjkMI0Q25HJtrXRNmtiwMPRwdZG5ImUI8/NDiI8PgKZumsbGRpkrIiJjDCNENiTn11/RqNcDAG67+WaZq1EOlUoltY5cunKF16khsjEMI0Q2xHi8yG19+shYifIYD2LlFF8i26KWuwAius54Js1tN9+MOpTJWI3921+4X7rv5Suk+1nHj+Mv48Z1TxFbt1p+zKRJnV8HkQ1jywiRDTG0jLi5uCD22gXeqHPEhAVBo+biZ0S2iGGEyEaUVVXht5ISAMCQ6Gho1Gy47EwuGjViwoIAACfPnkVZVZXMFRGRgVVhZPny5YiMjISrqyvi4+Px7bfftrpvZmYmVCpVi9txTq8jMnHAeLwIB692iQGRodL9fWwdIbIZFoeRjRs34vnnn8ecOXOQk5ODO+64A+PHj0f+DUannzhxAsXFxdKtDwfnEZkwGS/Cz0eXGBB1fXl9Ln5GZDssDiP/+Mc/8Oijj+Kxxx5Dv379sGTJEoSFhWHFihVtHhcQEICgoCDp5nztwlVE1GQ/W0a6XJxRywjDCJHtsCiM1NXV4eDBg0hOTjbZnpycjL1797Z57ODBgxEcHIykpCTs2rWrzX1ra2tRVVVlciNSMiGE1DLi6+GByMBAmStSpkBvT4T6+QFoCn8NXPyMyCZYFEbKysrQ2NiIwGa/KAMDA1FybeBdc8HBwXjvvfeQkZGBTZs2ISYmBklJSdizZ0+rr5Oeng6dTifdwsLCLCmTyO4cPX1aGlA5LCYGKpVK5oqUa/i1xc9qrl7FT9dWuyUieVk1XL/5L0ohRKu/PGNiYhBjtNhQYmIiCgoKsHjxYowYMcLsMWlpaUhNTZUeV1VVMZCQom3LzpbujxsyRMZKlC8xJgb//u47AE1dNbdERclcERFZ1DLi5+cHZ2fnFq0gpaWlLVpL2jJs2DD8YtQ/3pxWq4Wnp6fJjUjJth88KN2fkJAgYyXKx4vmEdkei8KIi4sL4uPjsXPnTpPtO3fuxPDhw9v9PDk5OQgO5tVIiQDg4qVL0h/FmF69EBUUJHNFyjY4KgpajQYAsPunnyCEuMERRNTVLO6mSU1NxbRp05CQkIDExES89957yM/Px8yZMwE0dbGcPXsWH374IQBgyZIl6N27N2JjY1FXV4d169YhIyMDGRkZnXsmRHbqq5wc6K9dHO9utop0OReNBqPi4vBlTg4Kyspw+LffMDg6Wu6yiByaxWFkypQpKC8vx2uvvYbi4mLExcVh+/btiIiIAAAUFxebrDlSV1eHWbNm4ezZs3Bzc0NsbCy2bduGCRMmdN5ZENmx7UbjRdhF0z3uHToUX+bkAAA+/+EHhhEimamEHbRRVlVVQafTobKykuNH6MasuTCZTPR6PYJSUnC+shI93dxQtm6d1IUAmF7ojTrHbaG34Wx5OUIfeQQAMCgyEoffflvmqprhhfJIIdr795sXvyCSUfapUzhfWQkAiL85HEfO5chckWPo5euLhJtuQvapUziSl4fT586hN9d2IZINL5RHJCPjLprhsewq6E73DRsm3d+yny1QRHJiGCGSkfGU3sTYm2SsxPHcO3SodP/zH36QsRIiYhghksm5ixelK/UOioxEoDfHQ3Wn2PBwaRr17p9+wsVLl2SuiMhxMYwQyWTHoUPS/Qnx8TJW4phUKpXUOtKo12PbgQMyV0TkuBhGiGTCVVfldx+7aohsAmfTkO2yoym6lmpobJTWufByd8ewmBgcKj54g6Oosw3v1w++Hh4or67Gjpwc1NbXm0ytlo21P/ucEkx2ii0jRDLIOn4clTU1AICxQ4ZA7ewsc0WOSe3sjIm33goAuHTlCr758UeZKyJyTGwZIepC5hYtuy30Nny2b5/0mONF5HXv0KH44JtvAACf7duH8Xw/iLodW0aIutmV2lqsvfbHz0Wt5ngRmSUPHgxXFxcATeuNGK4TRETdh2GEqJtt/O47XKiuBgA8cPvt8OMlDmTl7uqKMbfcAgAouXgR+0+elLcgIgfEMELUjYQQ+N9t26THT/GCkTbBeFbNyh07ZKyEyDExjBB1o59PF+HgqVMAgCHR0RgaEyNzRY5nf+H+FrcHbr8d3j17AgA+3rMHReXlMldJ5FgYRoi60ae7r1+L5qkJE6BSqWSshgx6urnhifHjAQD1DQ145z//kbkiIsfCMELUTS5W1+C/h44BAHw8PPDHESNkroiMPX333XBRN00wXLljBy5duSJzRUSOg2GEqJt8/v1h1Dc0AgDGD43F0fNHTLoKSD77C/ej4PIpJN8aCwCoqKnB3E9XyVwVkeNgGCHqBo16PTZ/17TCqkoFTB7BtSxs0dSkYdL99d/8gIbGRhmrIXIcXPSMqBt8d/QXlFyoAgAk9r8Jvfy8Za6IzIkK8Udi/2hkHfsVxeWV2JyVhf+5/Xa5y2o/a5aR5xLyZAPYMkLUDYwHrv7PKC5yZsum3nW9dWTxZ59BCCFjNUSOgWGEqIsdPpWP/cfzAACh/t4Y1i9a5oqoLQkxvXFzaCAAYP/Jk/g+N1fmioiUj2GEqAvV1Tcg/aPri5z9cfRQODlxOq8tU6lUeMho7MiCTz+VsRoix8AwQtSF1n75PU6fa1pAK7Z3CO6/Y4jMFVF7jEnojwAvDwDAtuxs/Pu772SuiEjZGEaIusjP+fn44MvvAQDOTk5Ie+huODvxI2cP1M7OePr+JOnxzOXLcZarshJ1Gc6moa5nzQh/O6fX6/Hn//1fNDQ2XQF22phE9Lk2DoHsw9hb4/DzqXJs/PZbXLx0CY8uXYov5s3jqrlEXYD/TSPqAit37MDe48cBAGEBPvjThDtkroissXzmTIT4+AAAvszJwYovvpC5IiJlYhgh6mRny8vx4gcfSI/THpoArYaNkPbIx8MDq599Vno8a/VqnCgslLEiImXib0hHZW3XCRdIatPFS5dw3xtvoPradU3uGX4L4m/uLW9R1CFjhwzBUxMmYNn27bhSV4fpS5bg+4ULoXZ2lrs0IsVgGCHqJOVVVRjz6qvI+e03AECwjw+e+X3SDY4ie7DokUew88gRnDx7FvtPnsSDb72FdampcHVxkbu0juN/TMgGsJuGqBOUVVUh6ZVXpCASoNNh52uvwbOHm8yVUWfoodViXWoqNNeu6puxdy/Gzp2LikuXZK6MSBlUwg7WOq6qqoJOp0NlZSU8PT3lLkcZHHCGS1cprajAXa+8gqNnzgAAgry98c3f/45+YWG8Gq+duy30NpPHXxw8iD8sWIDLtbUAgLiICOyYNw+9fH3lKM8+sUXFobT37zfDiKNiGOkUu3/6CX9etgwnz54FAPjrPLDs+YcREcg/TkqluuKNu197DecrKwEAYX5+2D53LuIiIlrsay6MNg84DodhxKG09+83u2mIrFBYVoY/vvUWRr30khREArw8sOKFaQwiCndrnz74fuFCRAY2rRtTUFaGIS+8gGfefRfnLl6UuToi+8QwQmSB6suX8ea//42YJ57Ahm+/lbbH33QTVqZOR1iAj4zVUXfpExKCrEWLMCS66aKH9Q0N+N9t2xD9l7/g1Y8+QmVNjcwVEtkXq8LI8uXLERkZCVdXV8THx+Nbo1/K5uzevRvx8fFwdXVFVFQUVq5caVWxRHK4dOUKNuzZg9+/+Sb8p03DnHXrpDEDfp6eeO+pp/DDW2+hl5+3zJVSdwr09sae9HTMeeAB9NBqAQA1V6/i9Y0bEfn443jk7bexK+c4Ll+tk7lSIttn8ZiRjRs3Ytq0aVi+fDl+97vf4d1338U///lPHDt2DOHh4S32z8vLQ1xcHB5//HH85S9/wffff48nn3wS69evx+TJk9v1mhwz0gaO/ehUjY2N+KW4GD+ePo0jeXnIPJaDgydPo7a+wWQ/J5UKfxiZgMcnjuCMGUK4ex+88e9/490vv0R9g+nPikbtjPibIzD4pnBEBvnjnlvuRHRwsMk6JRxbcgPdPc7Emt+rHAtjVpcNYB06dCiGDBmCFStWSNv69euH++67D+np6S32/9vf/oYtW7YgNzdX2jZz5kwcOXIEWVlZ7XrNrgojhUpYSXHnTrkrMMvacdHmjjI8l/G/wvCv0f1GvR6Nej301/6tb2xEfUMD6q7dauvrcenqVVy6cqXp36tXUVZVheILF1BSUYHiCxdw9sIFXK1r/X+y3h7uGD24LyaPiEd0SIBV50jKYwgOeSUlmL9hAz75/nup9cwcjVqNm4KDEejlhQCdDnrnq/Du2QM93Vzh6qKGm9YFA4P7o4dWC41aDY2zs/Svs5MTnJ2d4aRSNd2cnKACoFKpmm5G942Zu6ZOe66yYxPX4hkzpntfz5rfq91dYxdwdXWFn59fpz5ne/9+W7ToWV1dHQ4ePIgXX3zRZHtycjL27t1r9pisrCwkJyebbBs7dixWrVqF+vp6aDSaFsfU1tai1uiDXHlt1HpVVZUl5d5QWFhYpz4fKZNXzx4YMeBmjLzlZgyMCoPTtSvvXrrS+h8bcixVly8DAHw9PbH0z3/Gohkz8O2xY/hwzw5kHfsV5yuqTfavb2hAbkEBcgsK2njWz7quYCIzxo8fjw0bNnTqcxr+bt/oP6gWhZGysjI0NjYiMND06qOBgYEoKSkxe0xJSYnZ/RsaGlBWVobg4OAWx6Snp2P+/PkttjM8kBwqLl3GlqzD2JJ1WO5SiIi6zBdffAGdTtclz11dXd3mc1u1HHzzZjshRJtNeeb2N7fdIC0tDampqdJjvV6PCxcuwNfXt83XqaqqQlhYGAoKChQ3tkTJ5wYo+/x4bvZLyefHc7Nf9nR+QghUV1cjJCSkzf0sCiN+fn5wdnZu0QpSWlraovXDICgoyOz+arUavq2sWqjVaqG9NjrdwMvLq911enp62vwbZC0lnxug7PPjudkvJZ8fz81+2cv5tae1xaKpvS4uLoiPj8fOZoN7du7cieHDh5s9JjExscX+X331FRISEsyOFyEiIiLHYvE6I6mpqfjnP/+J1atXIzc3Fy+88ALy8/Mxc+ZMAE1dLNOnT5f2nzlzJs6cOYPU1FTk5uZi9erVWLVqFWbNmtV5Z0FERER2y+IxI1OmTEF5eTlee+01FBcXIy4uDtu3b0fEtesyFBcXIz8/X9o/MjIS27dvxwsvvIBly5YhJCQES5cubfcaI5bQarWYO3duiy4eJVDyuQHKPj+em/1S8vnx3OyXEs/PLi6UR0RERMrFa9MQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERycquwsgbb7yB4cOHo0ePHu1ekVUIgXnz5iEkJARubm4YNWoUfv75Z5N9amtr8cwzz8DPzw/u7u645557ZLmi78WLFzFt2jTodDrodDpMmzYNFRUVbR4jXamz2e2tt96S9hk1alSLrz/44INdfDamrDm3GTNmtKh72LBhJvvYwntn6bnV19fjb3/7GwYMGAB3d3eEhIRg+vTpKCoqMtlPrvdt+fLliIyMhKurK+Lj4/Htt9+2uf/u3bsRHx8PV1dXREVFYeXKlS32ycjIQP/+/aHVatG/f39s3ry5q8pvkyXntmnTJowZMwb+/v7w9PREYmIivvzyS5N91q5da/bzd/Xq1a4+FbMsOb/MzEyztR8/ftxkP3t878z97lCpVIiNjZX2sZX3bs+ePZg0aRJCQkKgUqnw2Wef3fAYe/rMtZuwI6+++qr4xz/+IVJTU4VOp2vXMQsWLBAeHh4iIyNDHD16VEyZMkUEBweLqqoqaZ+ZM2eKXr16iZ07d4pDhw6JO++8UwwaNEg0NDR00ZmYN27cOBEXFyf27t0r9u7dK+Li4sTEiRPbPKa4uNjktnr1aqFSqcSvv/4q7TNy5Ejx+OOPm+xXUVHR1adjwppzS0lJEePGjTOpu7y83GQfW3jvLD23iooKcdddd4mNGzeK48ePi6ysLDF06FARHx9vsp8c79uGDRuERqMR77//vjh27Jh47rnnhLu7uzhz5ozZ/X/77TfRo0cP8dxzz4ljx46J999/X2g0GvHpp59K++zdu1c4OzuLN998U+Tm5oo333xTqNVqsW/fvi49l+YsPbfnnntOLFy4UOzfv1+cPHlSpKWlCY1GIw4dOiTts2bNGuHp6dnicygHS89v165dAoA4ceKESe3Gnx17fe8qKipMzqmgoED4+PiIuXPnSvvYynu3fft2MWfOHJGRkSEAiM2bN7e5vz195ixhV2HEYM2aNe0KI3q9XgQFBYkFCxZI265evSp0Op1YuXKlEKLph1aj0YgNGzZI+5w9e1Y4OTmJHTt2dHrtrTl27JgAYPLDkpWVJQCI48ePt/t57r33XjF69GiTbSNHjhTPPfdcZ5VqMWvPLSUlRdx7772tft0W3rvOet/2798vAJj8cpXjfbvtttvEzJkzTbb17dtXvPjii2b3nz17tujbt6/Jtr/85S9i2LBh0uMHHnhAjBs3zmSfsWPHigcffLCTqm4fS8/NnP79+4v58+dLj9v7u6g7WHp+hjBy8eLFVp9TKe/d5s2bhUqlEqdPn5a22dJ7Z9CeMGJPnzlL2FU3jaXy8vJQUlKC5ORkaZtWq8XIkSOxd+9eAMDBgwdRX19vsk9ISAji4uKkfbpDVlYWdDodhg4dKm0bNmwYdDpdu+s4d+4ctm3bhkcffbTF1z766CP4+fkhNjYWs2bNQnV1dafVfiMdObfMzEwEBATg5ptvxuOPP47S0lLpa7bw3nXG+wYAlZWVUKlULbofu/N9q6urw8GDB02+nwCQnJzc6rlkZWW12H/s2LHIzs5GfX19m/t05+fLmnNrTq/Xo7q6Gj4+PibbL126hIiICISGhmLixInIycnptLrbqyPnN3jwYAQHByMpKQm7du0y+ZpS3rtVq1bhrrvuklYKN7CF985S9vKZs5TFy8HbE8PVgptfUTgwMBBnzpyR9nFxcYG3t3eLfZpfbbgrlZSUICAgoMX2gICAdtfxwQcfwMPDA7///e9Ntk+dOhWRkZEICgrCTz/9hLS0NBw5cqTFBQy7irXnNn78ePzP//wPIiIikJeXh1deeQWjR4/GwYMHodVqbeK964z37erVq3jxxRfx0EMPmVyBs7vft7KyMjQ2Npr9vLR2LiUlJWb3b2hoQFlZGYKDg1vdpzs/X9acW3P/9//+X9TU1OCBBx6QtvXt2xdr167FgAEDUFVVhbfffhu/+93vcOTIEfTp06dTz6Et1pxfcHAw3nvvPcTHx6O2thb/+te/kJSUhMzMTIwYMQJA6++vPb13xcXF+OKLL/Dxxx+bbLeV985S9vKZs5TsYWTevHmYP39+m/scOHAACQkJVr+GSqUyeSyEaLGtufbs0x7tPT+gZZ2W1rF69WpMnToVrq6uJtsff/xx6X5cXBz69OmDhIQEHDp0CEOGDGnXc5vT1ec2ZcoU6X5cXBwSEhIQERGBbdu2tQhcljxve3TX+1ZfX48HH3wQer0ey5cvN/laV71vN2Lp58Xc/s23W/MZ7ArW1rF+/XrMmzcPn3/+uUn4HDZsmMmg6t/97ncYMmQI3nnnHSxdurTzCm8nS84vJiYGMTEx0uPExEQUFBRg8eLFUhix9Dm7krV1rF27Fl5eXrjvvvtMttvae2cJe/rMtZfsYeTpp5++4QyB3r17W/XcQUFBAJqSZHBwsLS9tLRUSo1BQUGoq6vDxYsXTf6HXVpaiuHDh1v1usbae34//vgjzp071+Jr58+fb5Fwzfn2229x4sQJbNy48Yb7DhkyBBqNBr/88kuH/qh117kZBAcHIyIiAr/88guArn3vuuPc6uvr8cADDyAvLw/ffPONSauIOZ31vrXGz88Pzs7OLf73ZPx5aS4oKMjs/mq1Gr6+vm3uY8l731HWnJvBxo0b8eijj+KTTz7BXXfd1ea+Tk5OuPXWW6Wf0e7SkfMzNmzYMKxbt056bO/vnRACq1evxrRp0+Di4tLmvnK9d5ayl8+cxbp/mErHWTqAdeHChdK22tpaswNYN27cKO1TVFQk2wDWH374Qdq2b9++dg+ETElJaTEbozVHjx4VAMTu3butrtcSHT03g7KyMqHVasUHH3wghLCN987ac6urqxP33XefiI2NFaWlpe16re5432677TbxxBNPmGzr169fmwNY+/XrZ7Jt5syZLQbTjR8/3mSfcePGyTII0pJzE0KIjz/+WLi6ut5wUKGBXq8XCQkJ4pFHHulIqVax5vyamzx5srjzzjulx/b83glxfZDu0aNHb/gacr53BmjnAFZ7+cxZwq7CyJkzZ0ROTo6YP3++6Nmzp8jJyRE5OTmiurpa2icmJkZs2rRJerxgwQKh0+nEpk2bxNGjR8Uf//hHs1N7Q0NDxX//+19x6NAhMXr0aNmm9g4cOFBkZWWJrKwsMWDAgBZTRJufnxBCVFZWih49eogVK1a0eM5Tp06J+fPniwMHDoi8vDyxbds20bdvXzF48OBun/5qyblVV1eLv/71r2Lv3r0iLy9P7Nq1SyQmJopevXrZ3Htn6bnV19eLe+65R4SGhorDhw+bTCusra0VQsj3vhmmUK5atUocO3ZMPP/888Ld3V2ahfDiiy+KadOmSfsbphm+8MIL4tixY2LVqlUtphl+//33wtnZWSxYsEDk5uaKBQsWyDo9tL3n9vHHHwu1Wi2WLVvW6vTqefPmiR07dohff/1V5OTkiEceeUSo1WqTcGqr5/f//t//E5s3bxYnT54UP/30k3jxxRcFAJGRkSHtY6/vncHDDz8shg4davY5beW9q66ulv6WARD/+Mc/RE5OjjSzzp4/c5awqzCSkpIiALS47dq1S9oHgFizZo30WK/Xi7lz54qgoCCh1WrFiBEjWqTkK1euiKefflr4+PgINzc3MXHiRJGfn99NZ3VdeXm5mDp1qvDw8BAeHh5i6tSpLabdNT8/IYR49913hZubm9k1KPLz88WIESOEj4+PcHFxEdHR0eLZZ59tsV5HV7P03C5fviySk5OFv7+/0Gg0Ijw8XKSkpLR4X2zhvbP03PLy8sz+HBv/LMv5vi1btkxEREQIFxcXMWTIEJOWmJSUFDFy5EiT/TMzM8XgwYOFi4uL6N27t9lQ/Mknn4iYmBih0WhE3759Tf7gdSdLzm3kyJFm36OUlBRpn+eff16Eh4cLFxcX4e/vL5KTk8XevXu78YxMWXJ+CxcuFNHR0cLV1VV4e3uL22+/XWzbtq3Fc9rjeydEU8upm5ubeO+998w+n628d4bWm9Z+zuz9M9deKiGujXwhIiIikoGi1xkhIiIi28cwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGTFMEJERESyYhghIiIiWf1/j1+ULvozLQgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate random data from a normal distribution\n",
    "mu, sigma = 0, 0.1  # mean and standard deviation\n",
    "s = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "# Create the histogram\n",
    "plt.hist(s, bins=30, density=True, alpha=0.3, color='g')\n",
    "plt.hist(s / (np.abs(s).max()), bins=30, density=True, alpha=0.3, color='r')\n",
    "\n",
    "# Add a 'best fit' line\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = np.exp(-((x - mu)**2) / (2 * sigma**2)) / (sigma * np.sqrt(2 * np.pi))\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "\n",
    "title = \"Histogram of Normal Distribution\"\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
